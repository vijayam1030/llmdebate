# ğŸ¯ LLM Debate System - Angular Edition

**A modern AI debate system with Angular frontend and FastAPI backend using local Ollama models**

## ğŸš€ Quick Start

### **Windows (Easiest)**
```bash
# Double-click:
start_angular_ui.bat

# Or from command line:
python -m uvicorn api.main:app --host 0.0.0.0 --port 8000
```

### **Any Platform**
```bash
# Install Python dependencies:
pip install -r system/requirements.txt

# Install Angular dependencies (first time only):
cd angular-ui && npm install && cd ..

# Build Angular frontend (first time only):
cd angular-ui && npx ng build && cd ..

# Run the server:
python -m uvicorn api.main:app --host 0.0.0.0 --port 8000
```

## ğŸŒ Access the Application

Open your browser and go to: **http://localhost:8000**

## ğŸ“ Project Structure

```
ğŸ“ Root (Clean!)
â”œâ”€â”€ api/                        ğŸš€ FastAPI backend
â”‚   â””â”€â”€ main.py                    REST API server
â”œâ”€â”€ angular-ui/                 ğŸ¨ Angular frontend
â”‚   â”œâ”€â”€ src/                       Source code
â”‚   â””â”€â”€ dist/                      Built files (served by API)
â”œâ”€â”€ start_angular_ui.bat        â­ Main launcher
â”œâ”€â”€ system/                     ğŸ”§ Core system files
â”œâ”€â”€ backend/                    ğŸ¤– Debate logic
â”œâ”€â”€ ui/                         ğŸ“ Legacy Streamlit UIs
â”œâ”€â”€ legacy/                     ğŸ“ Old/archived files
â”œâ”€â”€ README.md                   ğŸ“– This file
â””â”€â”€ .env                        ğŸ”§ Environment config

ğŸ“ /api/                        ğŸš€ FastAPI Backend
â”œâ”€â”€ main.py                     ğŸŒ REST API server with endpoints

ğŸ“ /angular-ui/                 ğŸ¨ Angular Frontend  
â”œâ”€â”€ src/app/                    ğŸ“± Angular components
â”œâ”€â”€ dist/                       ğŸ“¦ Built files (auto-served)
â””â”€â”€ package.json                ğŸ“‹ Node dependencies

ğŸ“ /backend/                    ğŸ”§ Core system logic
â”œâ”€â”€ agents.py                   ğŸ¤– AI agents
â”œâ”€â”€ models.py                   ğŸ“Š Data models  
â”œâ”€â”€ debate_workflow.py          ğŸ”„ Debate flow
â”œâ”€â”€ consensus_engine.py         ğŸ¯ Agreement analysis
â””â”€â”€ ollama_integration.py       ğŸ”— Model interface

ğŸ“ /system/                     âš™ï¸ Configuration
â”œâ”€â”€ config.py                   ğŸ“‹ Settings
â”œâ”€â”€ dynamic_config.py           ğŸ”„ Auto-config
â”œâ”€â”€ main.py                     ğŸ¯ Core system
â””â”€â”€ requirements.txt            ğŸ“¦ Dependencies

ğŸ“ /bin/                        ğŸ’» Executables  
â”œâ”€â”€ launch.bat                  ğŸ–±ï¸ Easy Windows launcher
â”œâ”€â”€ launch_session_ui.bat       ğŸ¯ Session UI launcher
â””â”€â”€ *.bat, *.sh                 ğŸ”§ Other scripts

ğŸ“ /docs/                       ğŸ“š Documentation
â”œâ”€â”€ ORGANIZATION_COMPLETE.md    âœ… Structure guide
â”œâ”€â”€ WORKING_SETUP.md           ğŸ”§ Setup guide
â””â”€â”€ *.md                       ğŸ“– Other docs

ğŸ“ /logs/                       ğŸ“œ Log files
ğŸ“ /scripts/                    ğŸ”§ Utilities & tests
ğŸ“ /ui/                         ğŸ¨ Alternative UIs
ğŸ“ /legacy/                     ğŸ“¦ Archived code
```

## âœ¨ Key Features

- âœ… **Only Small Models** - Under 4GB each (tinyllama, phi3:mini, gemma2:2b, llama3.2:3b)
- âœ… **True Model Persistence** - Models stay loaded between debates  
- âœ… **Max 3 Rounds** - Efficient, focused debates
- âœ… **Clean Interface** - Streamlit web UI
- âœ… **Windows Compatible** - No Unicode/encoding issues
- âœ… **Organized Structure** - Clear separation of concerns

## ğŸ”§ Configuration

The system auto-configures using small models. Manual config in `/system/config.py` if needed.

**Recommended Models** (install with `ollama pull <model>`):
- `tinyllama:1.1b` - Ultra-fast, 1.1GB
- `phi3:mini` - Compact, 2.3GB  
- `gemma2:2b` - Efficient, 1.6GB
- `llama3.2:3b` - Balanced, 2GB

## ğŸ­ Usage

1. **Launch**: Double-click `bin\launch.bat` or run `streamlit run streamlit_app_session.py`
2. **Ask**: Enter your debate question
3. **Watch**: 3 AI agents debate the topic
4. **Results**: Get consensus analysis and summary

## ğŸ—ï¸ Development

- **Backend**: Core logic in `/backend/`
- **System**: Settings in `/system/`  
- **Tests**: Utilities in `/scripts/`
- **Docs**: Guides in `/docs/`

---

## âœ¨ Modern Angular Features

### ğŸ¨ **Beautiful Material Design UI**
- Clean, responsive interface with Angular Material components
- Real-time progress tracking with visual progress bars
- Tabbed results view for organized debate analysis
- Mobile-friendly responsive design

### ğŸš€ **Advanced Functionality** 
- **Real-time Updates**: Live debate progress with automatic polling
- **REST API**: Clean separation between frontend and backend
- **Session Management**: Persistent model loading and state
- **Error Handling**: User-friendly error messages and notifications
- **Expandable Results**: Collapsible rounds view for detailed analysis

### ğŸ”§ **Developer Benefits**
- **FastAPI Backend**: Modern async Python REST API
- **Angular Frontend**: TypeScript, reactive forms, HTTP client
- **Hot Reload**: Development server with live updates
- **Production Ready**: Optimized builds served directly by FastAPI
- **API Documentation**: Automatic Swagger/OpenAPI docs at `/docs`

## ğŸ¯ **Key Advantages Over Streamlit**

âœ… **Better Performance**: Faster loading and smoother interactions  
âœ… **Modern UI/UX**: Professional Material Design interface  
âœ… **Mobile Responsive**: Works perfectly on phones and tablets  
âœ… **Real-time Updates**: Live progress without page refreshes  
âœ… **Better Error Handling**: User-friendly error messages  
âœ… **API First**: RESTful design for easy integration  
âœ… **Production Ready**: Optimized builds and professional deployment

---

**Clean, fast, and efficient AI debates with local models! ğŸ‰**
