2025-06-28 18:49:36,891 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 18:49:36,892 - system.dynamic_config - INFO - Found 19 available models
2025-06-28 18:49:36,895 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 18:49:37,412 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 18:49:37,414 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['gemma2:2b', 'tinyllama:1.1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 18:49:37,929 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 18:49:37,932 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 18:49:37,932 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 18:49:40,098 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:49:40,099 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 18:49:40,099 - backend.ollama_integration - INFO - Model tinyllama:1.1b needs loading...
2025-06-28 18:49:40,099 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-28 18:49:41,394 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:49:41,395 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-28 18:49:41,395 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - skipping
2025-06-28 18:49:41,395 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 18:49:41,395 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 18:49:48,435 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:49:48,436 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 18:49:48,438 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 18:49:48,438 - system.main - INFO - System initialized successfully
2025-06-28 18:50:03,011 - system.main - INFO - Starting debate: how is lightning formed
2025-06-28 18:50:03,011 - backend.debate_workflow - INFO - Starting debate: how is lightning formed
2025-06-28 18:50:03,013 - backend.debate_workflow - INFO - Initializing debate for question: how is lightning formed
2025-06-28 18:50:03,014 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-28 18:50:03,014 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 18:50:03,208 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 18:50:03,380 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 18:50:05,604 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:50:05,605 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 18:50:10,513 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:50:10,514 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1658 chars
2025-06-28 18:50:14,366 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:50:14,366 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 1063 chars
2025-06-28 18:50:18,102 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:50:18,103 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 3182 chars
2025-06-28 18:50:18,103 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-28 18:50:18,104 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-28 18:50:18,193 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.741, reached=False
2025-06-28 18:50:18,194 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 18:50:18,236 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 18:50:25,177 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:50:25,178 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2328 chars
2025-06-28 18:50:25,178 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 18:50:25,179 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-28 18:50:25,179 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 18:50:25,363 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 18:50:25,535 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 18:50:38,657 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:50:38,657 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2153 chars
2025-06-28 18:50:50,878 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:50:50,878 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 4150 chars
2025-06-28 18:50:51,022 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:50:51,022 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2545 chars
2025-06-28 18:50:51,023 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-28 18:50:51,024 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-28 18:50:51,102 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.723, reached=False
2025-06-28 18:50:51,104 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 18:50:51,155 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 18:51:03,373 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:51:03,374 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2492 chars
2025-06-28 18:51:03,374 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 18:51:03,375 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-28 18:51:03,375 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 18:51:03,565 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 18:51:03,743 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 18:51:15,645 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:51:15,646 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3245 chars
2025-06-28 18:51:23,356 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:51:23,357 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2105 chars
2025-06-28 18:51:26,032 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:51:26,033 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2980 chars
2025-06-28 18:51:26,034 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-28 18:51:26,035 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-28 18:51:26,099 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.805, reached=False
2025-06-28 18:51:26,100 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-28 18:51:26,101 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 18:51:40,396 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 18:51:40,396 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3558 chars
2025-06-28 18:51:40,397 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-28 18:51:40,397 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-28 18:51:40,397 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-28 19:17:33,955 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 19:17:33,968 - api.main - ERROR - Failed to initialize system: cannot unpack non-iterable coroutine object
2025-06-28 19:17:33,968 - api.main - INFO - Angular static files mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:22:49,584 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 19:22:56,831 - api.main - INFO - Angular static files mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:22:56,835 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 19:22:56,836 - api.main - ERROR - Failed to initialize system: cannot unpack non-iterable coroutine object
2025-06-28 19:25:30,854 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 19:25:38,171 - api.main - INFO - Angular static files mounted from: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:26:58,087 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:26:58,561 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:26:58,561 - system.main - ERROR - Error during initialization: 'str' object has no attribute 'model'
2025-06-28 19:36:09,464 - api.main - INFO - Angular static files mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:36:09,469 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 19:36:09,967 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:36:09,970 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 19:36:09,975 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:36:10,460 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:36:10,464 - system.main - ERROR - Error during initialization: 'str' object has no attribute 'model'
2025-06-28 19:36:10,465 - api.main - INFO - System initialized successfully!
2025-06-28 19:38:10,100 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 19:38:20,527 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:38:20,530 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:38:20,531 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 19:38:21,034 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:38:21,036 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 19:38:21,040 - api.main - INFO - Configuration set - Orchestrator: phi3:mini
2025-06-28 19:38:21,040 - api.main - INFO - Configuration set - Debaters: ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 19:38:21,041 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:38:21,513 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:38:21,515 - system.main - ERROR - Error during initialization: 'str' object has no attribute 'model'
2025-06-28 19:38:21,516 - api.main - INFO - System initialized successfully!
2025-06-28 19:38:24,517 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 19:38:32,559 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:38:32,563 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:38:32,563 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 19:38:33,061 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:38:33,064 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 19:38:33,073 - api.main - INFO - Configuration set - Orchestrator: phi3:mini
2025-06-28 19:38:33,074 - api.main - INFO - Configuration set - Debaters: ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 19:38:33,075 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:38:33,601 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:38:33,604 - system.main - ERROR - Error during initialization: 'str' object has no attribute 'model'
2025-06-28 19:38:33,604 - api.main - INFO - System initialized successfully!
2025-06-28 19:38:51,081 - api.main - INFO - Status check - debate_system: True
2025-06-28 19:38:51,082 - api.main - ERROR - Error getting system status: type object 'Config' has no attribute 'ORCHESTRATOR_MAX_TOKENS'
2025-06-28 19:38:58,047 - api.main - INFO - Status check - debate_system: True
2025-06-28 19:38:58,048 - api.main - ERROR - Error getting system status: type object 'Config' has no attribute 'ORCHESTRATOR_MAX_TOKENS'
2025-06-28 19:39:01,202 - api.main - INFO - Status check - debate_system: True
2025-06-28 19:39:01,202 - api.main - ERROR - Error getting system status: type object 'Config' has no attribute 'ORCHESTRATOR_MAX_TOKENS'
2025-06-28 19:39:59,220 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:39:59,698 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:39:59,699 - system.main - ERROR - Error during initialization: 'str' object has no attribute 'model'
2025-06-28 19:41:46,891 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:41:47,356 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:41:47,358 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 19:41:47,836 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:41:47,839 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 19:41:47,839 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 19:41:49,353 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:41:49,353 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 19:41:49,353 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 19:41:49,354 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 19:41:51,554 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:41:51,555 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 19:41:51,555 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 19:41:51,555 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 19:41:53,682 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:41:53,683 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 19:41:53,683 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 19:41:53,683 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 19:41:56,152 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:41:56,152 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 19:41:56,155 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 19:41:56,155 - system.main - INFO - System initialized successfully
2025-06-28 19:42:27,643 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 19:42:39,044 - api.main - INFO - Angular dist folder found at: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:42:39,048 - api.main - INFO - Angular static files will be mounted from: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:42:39,049 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 19:42:39,524 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:42:39,524 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 19:42:39,527 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 19:42:39,527 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 19:42:39,528 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:42:39,990 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:42:39,991 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 19:42:40,462 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:42:40,465 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 19:42:40,466 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 19:42:42,633 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:42:42,633 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 19:42:42,634 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 19:42:42,634 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 19:42:44,889 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:42:44,889 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 19:42:44,890 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 19:42:44,890 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 19:42:46,914 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:42:46,915 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 19:42:46,915 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 19:42:46,915 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 19:42:49,382 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:42:49,382 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 19:42:49,387 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 19:42:49,387 - system.main - INFO - System initialized successfully
2025-06-28 19:42:49,387 - api.main - INFO - System initialized successfully!
2025-06-28 19:43:17,540 - api.main - INFO - Status check - debate_system: True
2025-06-28 19:43:17,541 - api.main - ERROR - Error getting system status: type object 'Config' has no attribute 'ORCHESTRATOR_MAX_TOKENS'
2025-06-28 19:43:28,735 - api.main - INFO - Status check - debate_system: True
2025-06-28 19:43:28,735 - api.main - ERROR - Error getting system status: type object 'Config' has no attribute 'ORCHESTRATOR_MAX_TOKENS'
2025-06-28 19:43:32,810 - api.main - INFO - Status check - debate_system: True
2025-06-28 19:43:32,811 - api.main - ERROR - Error getting system status: type object 'Config' has no attribute 'ORCHESTRATOR_MAX_TOKENS'
2025-06-28 19:44:28,835 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 19:44:40,750 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:44:41,224 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:44:41,226 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 19:44:41,700 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:44:41,701 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 19:44:41,702 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 19:44:43,846 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:44:43,846 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 19:44:43,847 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 19:44:43,847 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 19:44:46,109 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:44:46,110 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 19:44:46,110 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 19:44:46,110 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 19:44:48,210 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:44:48,211 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 19:44:48,211 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 19:44:48,211 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 19:44:50,711 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:44:50,712 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 19:44:50,715 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 19:44:50,715 - system.main - INFO - System initialized successfully
2025-06-28 19:45:13,958 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:45:14,437 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:45:14,438 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 19:45:14,915 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:45:14,916 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 19:45:14,917 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 19:45:17,093 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:45:17,093 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 19:45:17,094 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 19:45:17,094 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 19:45:19,320 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:45:19,321 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 19:45:19,321 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 19:45:19,321 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 19:45:21,466 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:45:21,467 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 19:45:21,467 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 19:45:21,467 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 19:45:23,907 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:45:23,908 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 19:45:23,910 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 19:45:23,911 - system.main - INFO - System initialized successfully
2025-06-28 19:46:13,887 - api.main - INFO - Angular dist folder found at: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:46:13,890 - api.main - INFO - Angular static files will be mounted from: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:46:13,891 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 19:46:14,365 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:46:14,368 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 19:46:14,376 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 19:46:14,376 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 19:46:14,376 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:46:14,850 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:46:14,851 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 19:46:15,318 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:46:15,319 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 19:46:15,319 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 19:46:17,471 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:46:17,471 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 19:46:17,471 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 19:46:17,472 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 19:46:19,703 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:46:19,704 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 19:46:19,704 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 19:46:19,704 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 19:46:21,749 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:46:21,750 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 19:46:21,750 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 19:46:21,750 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 19:46:24,320 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:46:24,320 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 19:46:24,325 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 19:46:24,325 - system.main - INFO - System initialized successfully
2025-06-28 19:46:24,325 - api.main - INFO - System initialized successfully!
2025-06-28 19:46:24,887 - api.main - INFO - Status check - debate_system: True
2025-06-28 19:46:24,889 - api.main - ERROR - Error getting system status: 4 validation errors for SystemStatusResponse
models_loaded.0
  Input should be a valid string [type=string_type, input_value=ModelConfig(name='Analyti...and logical reasoning.'), input_type=ModelConfig]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
models_loaded.1
  Input should be a valid string [type=string_type, input_value=ModelConfig(name='Creativ...d innovative thinking.'), input_type=ModelConfig]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
models_loaded.2
  Input should be a valid string [type=string_type, input_value=ModelConfig(name='Pragmat...al-world applications.'), input_type=ModelConfig]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
models_loaded.3
  Input should be a valid string [type=string_type, input_value=ModelConfig(name='Orchest...finding common ground.'), input_type=ModelConfig]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-28 19:46:29,093 - api.main - INFO - Status check - debate_system: True
2025-06-28 19:46:29,093 - api.main - ERROR - Error getting system status: 4 validation errors for SystemStatusResponse
models_loaded.0
  Input should be a valid string [type=string_type, input_value=ModelConfig(name='Analyti...and logical reasoning.'), input_type=ModelConfig]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
models_loaded.1
  Input should be a valid string [type=string_type, input_value=ModelConfig(name='Creativ...d innovative thinking.'), input_type=ModelConfig]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
models_loaded.2
  Input should be a valid string [type=string_type, input_value=ModelConfig(name='Pragmat...al-world applications.'), input_type=ModelConfig]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
models_loaded.3
  Input should be a valid string [type=string_type, input_value=ModelConfig(name='Orchest...finding common ground.'), input_type=ModelConfig]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-06-28 19:47:33,542 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 19:48:12,493 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:48:12,933 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:48:12,934 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 19:48:13,404 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:48:13,405 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 19:48:13,405 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 19:48:15,828 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:48:15,828 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 19:48:15,828 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 19:48:15,829 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 19:48:18,132 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:48:18,133 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 19:48:18,133 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 19:48:18,133 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 19:48:20,143 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:48:20,144 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 19:48:20,144 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 19:48:20,144 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 19:48:22,620 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:48:22,620 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 19:48:22,624 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 19:48:22,624 - system.main - INFO - System initialized successfully
2025-06-28 19:48:30,166 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:48:30,624 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:48:30,627 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 19:48:31,110 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:48:31,111 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 19:48:31,111 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 19:48:33,184 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:48:33,184 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 19:48:33,184 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 19:48:33,184 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 19:48:35,344 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:48:35,345 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 19:48:35,345 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 19:48:35,345 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 19:48:37,472 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:48:37,473 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 19:48:37,473 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 19:48:37,473 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 19:48:39,901 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:48:39,902 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 19:48:39,905 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 19:48:39,905 - system.main - INFO - System initialized successfully
2025-06-28 19:48:54,575 - api.main - INFO - Angular dist folder found at: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:48:54,579 - api.main - INFO - Angular static files will be mounted from: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:48:54,587 - httpx - INFO - HTTP Request: GET http://testserver/ "HTTP/1.1 200 OK"
2025-06-28 19:48:54,589 - api.main - INFO - Status check - debate_system: False
2025-06-28 19:48:54,590 - httpx - INFO - HTTP Request: GET http://testserver/api/status "HTTP/1.1 200 OK"
2025-06-28 19:51:21,326 - api.main - INFO - Angular dist folder found at: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:51:21,329 - api.main - INFO - Angular static files will be mounted from: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:51:21,330 - api.main - INFO - Status check - debate_system: False
2025-06-28 19:52:02,013 - api.main - INFO - Angular dist folder found at: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:52:02,017 - api.main - INFO - Angular static files will be mounted from: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:52:02,018 - api.main - INFO - Status check - debate_system: False
2025-06-28 19:52:46,597 - api.main - INFO - Angular dist folder found at: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:52:46,601 - api.main - INFO - Angular static files will be mounted from: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:53:57,551 - api.main - INFO - Angular dist folder found at: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:53:57,554 - api.main - INFO - Angular static files will be mounted from: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:53:58,046 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:53:58,048 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 19:53:58,053 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:53:58,526 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:53:58,528 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 19:53:58,991 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:53:58,992 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 19:53:58,992 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 19:54:00,433 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:54:00,433 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 19:54:00,434 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 19:54:00,434 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 19:54:02,635 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:54:02,635 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 19:54:02,635 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 19:54:02,635 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 19:54:04,727 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:54:04,727 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 19:54:04,728 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 19:54:04,728 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 19:54:07,204 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:54:07,205 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 19:54:07,208 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 19:54:07,208 - system.main - INFO - System initialized successfully
2025-06-28 19:54:07,209 - api.main - INFO - Status check - debate_system: True
2025-06-28 19:56:05,353 - api.main - INFO - Angular dist folder found at: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:56:05,357 - api.main - INFO - Angular static files will be mounted from: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:56:05,358 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 19:56:05,824 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:56:05,825 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 19:56:05,826 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 19:56:05,826 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 19:56:05,826 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:56:06,274 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:56:06,274 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 19:56:06,723 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:56:06,723 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 19:56:06,724 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 19:56:08,707 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:56:08,707 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 19:56:08,708 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 19:56:08,708 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 19:56:10,842 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:56:10,842 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 19:56:10,842 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 19:56:10,843 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 19:56:12,852 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:56:12,852 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 19:56:12,853 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 19:56:12,853 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 19:56:15,257 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:56:15,257 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 19:56:15,262 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 19:56:15,262 - system.main - INFO - System initialized successfully
2025-06-28 19:56:15,262 - api.main - INFO - System initialized successfully!
2025-06-28 19:56:38,450 - api.main - INFO - Status check - debate_system: True
2025-06-28 19:57:20,314 - api.main - INFO - Status check - debate_system: True
2025-06-28 19:57:36,353 - api.main - ERROR - Debate 4bb286a2-d95e-4a91-9ad3-f047c786424a failed: 'LLMDebateSystem' object has no attribute 'run_debate'
2025-06-28 19:58:13,783 - api.main - ERROR - Debate 38e9f622-19bc-4c72-a3b6-48c8bf3510d1 failed: 'LLMDebateSystem' object has no attribute 'run_debate'
2025-06-28 19:58:36,281 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 19:58:44,097 - api.main - INFO - Angular dist folder found at: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:58:44,101 - api.main - INFO - Angular static files will be mounted from: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:58:44,101 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 19:58:44,591 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:58:44,592 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 19:58:44,594 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 19:58:44,594 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 19:58:44,594 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:58:45,072 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:58:45,073 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 19:58:45,538 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:58:45,538 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 19:58:45,538 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 19:58:47,867 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:58:47,868 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 19:58:47,868 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 19:58:47,868 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 19:58:50,343 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:58:50,344 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 19:58:50,344 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 19:58:50,344 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 19:58:52,466 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:58:52,466 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 19:58:52,466 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 19:58:52,467 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 19:58:54,961 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:58:54,962 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 19:58:54,966 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 19:58:54,966 - system.main - INFO - System initialized successfully
2025-06-28 19:58:54,966 - api.main - INFO - System initialized successfully!
2025-06-28 19:59:23,995 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 19:59:31,123 - api.main - INFO - Angular dist folder found at: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:59:31,126 - api.main - INFO - Angular static files will be mounted from: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 19:59:31,127 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 19:59:31,596 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:59:31,597 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 19:59:31,598 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 19:59:31,598 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 19:59:31,598 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 19:59:32,035 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:59:32,036 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 19:59:32,466 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 19:59:32,467 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 19:59:32,467 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 19:59:34,490 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:59:34,490 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 19:59:34,491 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 19:59:34,491 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 19:59:36,551 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:59:36,552 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 19:59:36,552 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 19:59:36,552 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 19:59:38,562 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:59:38,563 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 19:59:38,563 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 19:59:38,563 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 19:59:40,953 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 19:59:40,953 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 19:59:40,958 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 19:59:40,958 - system.main - INFO - System initialized successfully
2025-06-28 19:59:40,958 - api.main - INFO - System initialized successfully!
2025-06-28 20:03:42,751 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:13:41,473 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:13:41,477 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:13:41,478 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:13:41,981 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:13:41,982 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:13:41,984 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:13:41,984 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:13:41,985 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:13:42,433 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:13:42,434 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:13:42,879 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:13:42,880 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:13:42,880 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:13:44,255 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:13:44,255 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:13:44,256 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:13:44,256 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:13:46,374 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:13:46,374 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:13:46,374 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:13:46,374 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:13:48,416 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:13:48,416 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:13:48,417 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:13:48,417 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:13:50,812 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:13:50,813 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:13:50,817 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:13:50,817 - system.main - INFO - System initialized successfully
2025-06-28 20:13:50,817 - api.main - INFO - System initialized successfully!
2025-06-28 20:15:26,261 - api.main - INFO - Status check - debate_system: True
2025-06-28 20:15:36,209 - system.main - INFO - Starting debate: should ai be regulated
2025-06-28 20:15:36,210 - backend.debate_workflow - INFO - Starting debate: should ai be regulated
2025-06-28 20:15:36,212 - backend.debate_workflow - INFO - Initializing debate for question: should ai be regulated
2025-06-28 20:15:36,213 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-28 20:15:36,213 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:15:36,406 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:15:36,585 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-28 20:15:41,581 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:15:41,581 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 1138 chars
2025-06-28 20:15:44,245 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:15:44,245 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-28 20:15:49,505 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:15:49,505 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1683 chars
2025-06-28 20:15:49,791 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:15:49,791 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2461 chars
2025-06-28 20:15:49,792 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-28 20:15:49,793 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-28 20:15:49,869 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.636, reached=False
2025-06-28 20:15:49,870 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 20:15:49,927 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:16:02,363 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:16:02,363 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2384 chars
2025-06-28 20:16:02,364 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 20:16:02,364 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-28 20:16:02,365 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:16:02,530 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:16:02,697 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:16:17,965 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:16:17,966 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 4181 chars
2025-06-28 20:16:23,522 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:16:23,522 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1063 chars
2025-06-28 20:16:27,211 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:16:27,212 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3008 chars
2025-06-28 20:16:27,212 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-28 20:16:27,213 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-28 20:16:27,295 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.572, reached=False
2025-06-28 20:16:27,296 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 20:16:27,355 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:16:39,981 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:16:39,982 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2474 chars
2025-06-28 20:16:39,982 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 20:16:39,983 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-28 20:16:39,983 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:16:40,154 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:16:40,323 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:16:48,966 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:16:48,967 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2196 chars
2025-06-28 20:16:54,251 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:16:58,249 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:16:58,250 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3071 chars
2025-06-28 20:16:58,251 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-28 20:16:58,252 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-28 20:16:58,400 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.646, reached=False
2025-06-28 20:16:58,402 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-28 20:16:58,403 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:17:11,291 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:17:11,292 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2663 chars
2025-06-28 20:17:11,293 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-28 20:17:11,294 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-28 20:17:11,294 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-28 20:17:11,294 - api.main - ERROR - Debate 5ce92c7d-d8b4-47f6-9cee-2645edce41ff failed: 'DebateResult' object has no attribute 'consensus_reached'
2025-06-28 20:18:02,815 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:18:10,075 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:18:10,078 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:18:10,079 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:18:10,535 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:18:10,536 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:18:10,537 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:18:10,537 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:18:10,538 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:18:10,972 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:18:10,972 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:18:11,402 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:18:11,402 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:18:11,403 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:18:13,109 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:18:13,109 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:18:13,110 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:18:13,110 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:18:15,223 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:18:15,224 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:18:15,224 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:18:15,224 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:18:17,251 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:18:17,252 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:18:17,252 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:18:17,252 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:18:19,627 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:18:19,628 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:18:19,628 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'llama3.2:1b', 'phi3:mini', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-28 20:18:19,628 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:18:19,628 - system.main - INFO - System initialized successfully
2025-06-28 20:18:19,629 - api.main - INFO - System initialized successfully!
2025-06-28 20:18:23,515 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:18:30,977 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:18:30,981 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:18:30,982 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:18:31,454 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:18:31,455 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:18:31,456 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:18:31,456 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:18:31,456 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:18:31,887 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:18:31,888 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:18:32,337 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:18:32,338 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:18:32,338 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:18:34,673 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:18:34,673 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:18:34,673 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:18:34,673 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:18:36,795 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:18:36,795 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:18:36,795 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:18:36,795 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:18:38,800 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:18:38,801 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:18:38,801 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:18:38,801 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:18:41,231 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:18:41,232 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:18:41,232 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:3b', 'phi3:mini', 'llama3.2:1b'] - PERSISTENT IN MEMORY
2025-06-28 20:18:41,232 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:18:41,232 - system.main - INFO - System initialized successfully
2025-06-28 20:18:41,232 - api.main - INFO - System initialized successfully!
2025-06-28 20:18:42,211 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:18:49,732 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:18:49,736 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:18:49,736 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:18:50,239 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:18:50,241 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:18:50,246 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:18:50,247 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:18:50,247 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:18:50,711 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:18:50,712 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:18:51,152 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:18:51,153 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:18:51,153 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:18:53,271 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:18:53,272 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:18:53,272 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:18:53,272 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:18:55,476 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:18:55,476 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:18:55,477 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:18:55,477 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:18:57,545 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:18:57,546 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:18:57,546 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:18:57,546 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:18:59,951 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:18:59,952 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:18:59,952 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'gemma2:2b', 'llama3.2:1b', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-28 20:18:59,952 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:18:59,952 - system.main - INFO - System initialized successfully
2025-06-28 20:18:59,952 - api.main - INFO - System initialized successfully!
2025-06-28 20:19:07,177 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:19:07,181 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:19:07,184 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:19:07,637 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:19:07,638 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:19:07,641 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:19:07,643 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:19:07,644 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:19:08,085 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:19:08,086 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:19:08,513 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:19:08,514 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:19:08,514 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:19:10,558 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:19:10,558 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:19:10,558 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:19:10,558 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:19:12,580 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:19:12,581 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:19:12,581 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:19:12,581 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:19:14,557 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:19:14,558 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:19:14,558 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:19:14,558 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:19:16,950 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:19:16,951 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:19:16,951 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-28 20:19:16,951 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:19:16,951 - system.main - INFO - System initialized successfully
2025-06-28 20:19:16,952 - api.main - INFO - System initialized successfully!
2025-06-28 20:19:16,953 - system.main - INFO - Starting debate: should ai be regulated
2025-06-28 20:19:16,953 - backend.debate_workflow - INFO - Starting debate: should ai be regulated
2025-06-28 20:19:16,955 - backend.debate_workflow - INFO - Initializing debate for question: should ai be regulated
2025-06-28 20:19:16,956 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-28 20:19:16,956 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:19:17,150 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:19:17,330 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-28 20:19:22,066 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:19:24,915 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:19:24,915 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-28 20:19:28,620 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:19:28,621 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1505 chars
2025-06-28 20:19:29,032 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:19:29,033 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1989 chars
2025-06-28 20:19:29,034 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-28 20:19:29,035 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-28 20:19:29,208 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.745, reached=False
2025-06-28 20:19:29,209 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 20:19:29,357 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:19:41,955 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:19:41,955 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2376 chars
2025-06-28 20:19:41,956 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 20:19:41,956 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-28 20:19:41,957 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:19:42,134 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:19:42,300 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:19:55,481 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:19:55,482 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3752 chars
2025-06-28 20:20:02,572 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:20:02,573 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1861 chars
2025-06-28 20:20:05,069 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:20:05,070 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2694 chars
2025-06-28 20:20:05,070 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-28 20:20:05,071 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-28 20:20:05,136 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.780, reached=False
2025-06-28 20:20:05,137 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 20:20:05,194 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:20:17,530 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:20:17,531 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2263 chars
2025-06-28 20:20:17,531 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 20:20:17,532 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-28 20:20:17,532 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:20:17,704 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:20:17,871 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:20:25,296 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:20:25,296 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 1825 chars
2025-06-28 20:20:33,410 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:20:33,411 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2192 chars
2025-06-28 20:20:37,248 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:20:37,249 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3618 chars
2025-06-28 20:20:37,249 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-28 20:20:37,250 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-28 20:20:37,315 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.839, reached=False
2025-06-28 20:20:37,316 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-28 20:20:37,317 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:20:51,002 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:20:51,002 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3030 chars
2025-06-28 20:20:51,003 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-28 20:20:51,003 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-28 20:20:51,003 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-28 20:20:51,003 - api.main - ERROR - Debate 228f8e66-1b56-4b72-b777-37b893a32838 failed: 'DebateRound' object has no attribute 'responses'
2025-06-28 20:20:51,026 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:20:58,312 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:20:58,316 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:20:58,316 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:20:58,781 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:20:58,781 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:20:58,783 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:20:58,783 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:20:58,783 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:20:59,215 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:20:59,215 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:20:59,714 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:20:59,714 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:20:59,714 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:21:01,468 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:21:01,468 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:21:01,469 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:21:01,469 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:21:03,693 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:21:03,693 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:21:03,694 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:21:03,694 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:21:05,782 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:21:05,782 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:21:05,782 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:21:05,783 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:21:08,269 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:21:08,269 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:21:08,269 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:1b', 'phi3:mini', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-28 20:21:08,269 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:21:08,269 - system.main - INFO - System initialized successfully
2025-06-28 20:21:08,270 - api.main - INFO - System initialized successfully!
2025-06-28 20:21:15,558 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:21:15,562 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:21:15,563 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:21:16,073 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:21:16,073 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:21:16,075 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:21:16,075 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:21:16,075 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:21:16,535 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:21:16,536 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:21:17,000 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:21:17,001 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:21:17,001 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:21:19,072 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:21:19,073 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:21:19,073 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:21:19,073 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:21:21,166 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:21:21,167 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:21:21,167 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:21:21,167 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:21:23,193 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:21:23,193 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:21:23,194 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:21:23,194 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:21:25,606 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:21:25,606 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:21:25,606 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'gemma2:2b', 'llama3.2:1b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-28 20:21:25,607 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:21:25,607 - system.main - INFO - System initialized successfully
2025-06-28 20:21:25,607 - api.main - INFO - System initialized successfully!
2025-06-28 20:22:01,254 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:22:08,713 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:22:08,716 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:22:08,717 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:22:09,179 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:22:09,180 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:22:09,181 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:22:09,182 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:22:09,182 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:22:09,613 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:22:09,614 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:22:10,058 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:22:10,059 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:22:10,059 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:22:12,092 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:22:12,092 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:22:12,092 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:22:12,093 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:22:14,271 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:22:14,271 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:22:14,271 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:22:14,271 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:22:16,323 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:22:16,324 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:22:16,324 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:22:16,324 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:22:18,765 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:22:18,766 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:22:18,766 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:3b', 'llama3.2:1b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-28 20:22:18,766 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:22:18,766 - system.main - INFO - System initialized successfully
2025-06-28 20:22:18,766 - api.main - INFO - System initialized successfully!
2025-06-28 20:22:22,486 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:22:29,819 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:22:29,823 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:22:29,824 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:22:30,296 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:22:30,297 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:22:30,299 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:22:30,299 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:22:30,299 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:22:30,740 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:22:30,741 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:22:31,197 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:22:31,197 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:22:31,198 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:22:33,476 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:22:33,476 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:22:33,477 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:22:33,477 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:22:35,615 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:22:35,615 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:22:35,615 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:22:35,615 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:22:37,692 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:22:37,692 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:22:37,693 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:22:37,693 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:22:40,063 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:22:40,064 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:22:40,064 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'gemma2:2b', 'llama3.2:3b', 'llama3.2:1b'] - PERSISTENT IN MEMORY
2025-06-28 20:22:40,064 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:22:40,064 - system.main - INFO - System initialized successfully
2025-06-28 20:22:40,064 - api.main - INFO - System initialized successfully!
2025-06-28 20:23:08,185 - api.main - INFO - Status check - debate_system: True
2025-06-28 20:23:16,755 - api.main - INFO - Starting debate 492947f0-76c7-4d45-bc9d-1dabce948ada: should ai be regulated
2025-06-28 20:23:16,755 - system.main - INFO - Starting debate: should ai be regulated
2025-06-28 20:23:16,755 - backend.debate_workflow - INFO - Starting debate: should ai be regulated
2025-06-28 20:23:16,757 - backend.debate_workflow - INFO - Initializing debate for question: should ai be regulated
2025-06-28 20:23:16,758 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-28 20:23:16,758 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:23:16,948 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:23:17,135 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-28 20:23:20,199 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:23:22,887 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:23:22,888 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-28 20:23:27,444 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:23:27,445 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1896 chars
2025-06-28 20:23:27,751 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:23:27,752 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1905 chars
2025-06-28 20:23:27,752 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-28 20:23:27,753 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-28 20:23:27,817 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.764, reached=False
2025-06-28 20:23:27,819 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 20:23:27,877 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:23:39,997 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:23:39,998 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2401 chars
2025-06-28 20:23:39,998 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 20:23:39,999 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-28 20:23:39,999 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:23:40,179 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:23:40,357 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:23:52,805 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:23:52,806 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3432 chars
2025-06-28 20:23:58,616 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:23:58,616 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1353 chars
2025-06-28 20:24:01,654 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:24:01,654 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2729 chars
2025-06-28 20:24:01,655 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-28 20:24:01,655 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-28 20:24:01,711 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.776, reached=False
2025-06-28 20:24:01,712 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 20:24:01,759 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:24:14,371 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:24:14,371 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2602 chars
2025-06-28 20:24:14,372 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 20:24:14,372 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-28 20:24:14,373 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:24:14,550 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:24:14,729 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:24:27,489 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:24:27,490 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3526 chars
2025-06-28 20:24:38,897 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:24:38,898 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 3431 chars
2025-06-28 20:24:42,203 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:24:42,203 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 4064 chars
2025-06-28 20:24:42,204 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-28 20:24:42,204 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-28 20:24:42,262 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.698, reached=False
2025-06-28 20:24:42,263 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-28 20:24:42,264 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:24:55,867 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:24:55,868 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3275 chars
2025-06-28 20:24:55,868 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-28 20:24:55,869 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-28 20:24:55,869 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-28 20:24:55,869 - api.main - ERROR - Debate 492947f0-76c7-4d45-bc9d-1dabce948ada failed: 'DebateRound' object has no attribute 'responses'
2025-06-28 20:25:09,717 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:25:16,907 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:25:16,911 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:25:16,912 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:25:17,370 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:25:17,370 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:25:17,371 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:25:17,372 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:25:17,372 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:25:17,803 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:25:17,803 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:25:18,238 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:25:18,239 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:25:18,239 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:25:20,224 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:25:20,224 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:25:20,224 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:25:20,224 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:25:22,287 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:25:22,287 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:25:22,288 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:25:22,288 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:25:24,197 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:25:24,197 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:25:24,197 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:25:24,198 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:25:26,506 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:25:26,506 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:25:26,506 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:1b', 'phi3:mini', 'gemma2:2b', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-28 20:25:26,506 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:25:26,507 - system.main - INFO - System initialized successfully
2025-06-28 20:25:26,507 - api.main - INFO - System initialized successfully!
2025-06-28 20:25:26,508 - api.main - INFO - Starting debate fdb99814-2ebc-40bc-a01c-d0793c5c1c9b: should ai be regulated
2025-06-28 20:25:26,508 - system.main - INFO - Starting debate: should ai be regulated
2025-06-28 20:25:26,509 - backend.debate_workflow - INFO - Starting debate: should ai be regulated
2025-06-28 20:25:26,511 - backend.debate_workflow - INFO - Initializing debate for question: should ai be regulated
2025-06-28 20:25:26,511 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-28 20:25:26,512 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:25:26,691 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:25:26,860 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-28 20:25:30,394 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:25:33,028 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:25:33,028 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-28 20:25:37,219 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:25:37,220 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1540 chars
2025-06-28 20:25:37,493 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:25:37,494 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2220 chars
2025-06-28 20:25:37,494 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-28 20:25:37,495 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-28 20:25:37,565 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.700, reached=False
2025-06-28 20:25:37,567 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 20:25:37,619 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:25:49,171 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:25:49,172 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2124 chars
2025-06-28 20:25:49,172 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 20:25:49,173 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-28 20:25:49,173 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:25:49,346 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:25:49,534 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:25:58,123 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:25:58,124 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2179 chars
2025-06-28 20:26:03,461 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:26:03,461 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1218 chars
2025-06-28 20:26:08,857 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:26:08,858 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3911 chars
2025-06-28 20:26:08,859 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-28 20:26:08,859 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-28 20:26:08,929 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.663, reached=False
2025-06-28 20:26:08,930 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 20:26:08,975 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:26:21,163 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:26:21,164 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2461 chars
2025-06-28 20:26:21,164 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 20:26:21,165 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-28 20:26:21,165 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:26:21,342 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:26:21,518 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:26:30,632 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:26:30,632 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2309 chars
2025-06-28 20:26:44,153 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:26:44,154 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 5119 chars
2025-06-28 20:26:45,365 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:26:45,366 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3590 chars
2025-06-28 20:26:45,366 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-28 20:26:45,367 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-28 20:26:45,426 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.717, reached=False
2025-06-28 20:26:45,427 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-28 20:26:45,427 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:26:58,991 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:26:58,992 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3414 chars
2025-06-28 20:26:58,992 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-28 20:26:58,993 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-28 20:26:58,993 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-28 20:26:58,993 - api.main - ERROR - Debate fdb99814-2ebc-40bc-a01c-d0793c5c1c9b failed: 'DebateRound' object has no attribute 'responses'
2025-06-28 20:29:47,054 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:29:54,628 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:29:54,631 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:29:54,632 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:29:55,111 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:29:55,112 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:29:55,114 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:29:55,115 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:29:55,115 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:29:55,570 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:29:55,570 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:29:56,011 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:29:56,012 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:29:56,012 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:29:58,034 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:29:58,034 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:29:58,034 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:29:58,035 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:30:00,143 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:30:00,144 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:30:00,144 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:30:00,144 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:30:02,199 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:30:02,200 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:30:02,200 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:30:02,200 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:30:04,494 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:30:04,495 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:30:04,495 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:1b', 'llama3.2:3b', 'gemma2:2b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-28 20:30:04,495 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:30:04,495 - system.main - INFO - System initialized successfully
2025-06-28 20:30:04,495 - api.main - INFO - System initialized successfully!
2025-06-28 20:31:24,284 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:31:31,689 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:31:31,693 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:31:31,693 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:31:32,151 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:31:32,152 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:31:32,153 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:31:32,153 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:31:32,153 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:31:32,586 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:31:32,587 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:31:33,070 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:31:33,071 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:31:33,071 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:31:35,064 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:31:35,065 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:31:35,065 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:31:35,065 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:31:37,176 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:31:37,176 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:31:37,177 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:31:37,177 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:31:39,194 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:31:39,195 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:31:39,195 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:31:39,195 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:31:41,580 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:31:41,581 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:31:41,581 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:3b', 'llama3.2:1b', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-28 20:31:41,581 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:31:41,581 - system.main - INFO - System initialized successfully
2025-06-28 20:31:41,581 - api.main - INFO - System initialized successfully!
2025-06-28 20:31:58,635 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:32:06,028 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:32:06,031 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:32:06,032 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:32:06,505 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:32:06,508 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:32:06,515 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:32:06,517 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:32:06,517 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:32:07,000 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:32:07,002 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:32:07,511 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:32:07,513 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:32:07,513 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:32:09,649 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:32:09,649 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:32:09,650 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:32:09,650 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:32:11,919 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:32:11,919 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:32:11,920 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:32:11,920 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:32:13,955 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:32:13,955 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:32:13,955 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:32:13,955 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:32:16,471 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:32:16,472 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:32:16,472 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'llama3.2:1b', 'gemma2:2b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-28 20:32:16,472 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:32:16,472 - system.main - INFO - System initialized successfully
2025-06-28 20:32:16,472 - api.main - INFO - System initialized successfully!
2025-06-28 20:32:23,665 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:32:23,669 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:32:23,669 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:32:24,136 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:32:24,137 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:32:24,138 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:32:24,139 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:32:24,139 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:32:24,585 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:32:24,585 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:32:25,032 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:32:25,032 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:32:25,032 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:32:27,055 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:32:27,056 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:32:27,056 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:32:27,056 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:32:29,190 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:32:29,190 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:32:29,190 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:32:29,191 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:32:31,210 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:32:31,210 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:32:31,210 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:32:31,211 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:32:33,520 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:32:33,521 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:32:33,521 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'phi3:mini', 'llama3.2:1b', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-28 20:32:33,521 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:32:33,521 - system.main - INFO - System initialized successfully
2025-06-28 20:32:33,521 - api.main - INFO - System initialized successfully!
2025-06-28 20:32:37,247 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:32:44,671 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:32:44,676 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:32:44,677 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:32:45,166 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:32:45,167 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:32:45,168 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:32:45,168 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:32:45,168 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:32:45,637 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:32:45,637 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:32:46,101 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:32:46,102 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:32:46,102 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:32:48,171 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:32:48,171 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:32:48,171 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:32:48,171 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:32:50,363 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:32:50,363 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:32:50,363 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:32:50,363 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:32:52,354 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:32:52,354 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:32:52,354 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:32:52,354 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:32:54,766 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:32:54,766 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:32:54,766 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:1b', 'gemma2:2b', 'phi3:mini', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-28 20:32:54,767 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:32:54,767 - system.main - INFO - System initialized successfully
2025-06-28 20:32:54,767 - api.main - INFO - System initialized successfully!
2025-06-28 20:33:02,412 - api.main - INFO - Starting debate 6e634b60-627d-4af1-b4aa-f155f18aad83: should ai be regulated
2025-06-28 20:33:02,413 - system.main - INFO - Starting debate: should ai be regulated
2025-06-28 20:33:02,413 - backend.debate_workflow - INFO - Starting debate: should ai be regulated
2025-06-28 20:33:02,415 - backend.debate_workflow - INFO - Initializing debate for question: should ai be regulated
2025-06-28 20:33:02,416 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-28 20:33:02,416 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:33:02,616 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:33:02,812 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-28 20:33:07,425 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:33:10,092 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:33:10,093 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-28 20:33:16,539 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:33:16,540 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2092 chars
2025-06-28 20:33:17,003 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:33:17,004 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 3270 chars
2025-06-28 20:33:17,004 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-28 20:33:17,005 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-28 20:33:17,091 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.640, reached=False
2025-06-28 20:33:17,092 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 20:33:17,138 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:33:27,852 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:33:27,853 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2254 chars
2025-06-28 20:33:27,853 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 20:33:27,854 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-28 20:33:27,854 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:33:28,029 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:33:28,226 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:33:41,557 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:33:41,557 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3699 chars
2025-06-28 20:33:53,823 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:33:53,824 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 3727 chars
2025-06-28 20:33:54,661 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:33:54,662 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2908 chars
2025-06-28 20:33:54,662 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-28 20:33:54,663 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-28 20:33:54,717 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.568, reached=False
2025-06-28 20:33:54,718 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 20:33:54,777 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:34:06,836 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:34:06,836 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2783 chars
2025-06-28 20:34:06,837 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 20:34:06,837 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-28 20:34:06,837 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:34:07,009 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:34:07,181 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:34:16,039 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:34:16,040 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2171 chars
2025-06-28 20:34:25,854 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:34:25,854 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2793 chars
2025-06-28 20:34:29,671 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:34:29,671 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 4015 chars
2025-06-28 20:34:29,672 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-28 20:34:29,672 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-28 20:34:29,726 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.692, reached=False
2025-06-28 20:34:29,727 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-28 20:34:29,727 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:34:42,073 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:34:42,074 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2588 chars
2025-06-28 20:34:42,074 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-28 20:34:42,074 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-28 20:34:42,075 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-28 20:34:42,075 - api.main - ERROR - Debate 6e634b60-627d-4af1-b4aa-f155f18aad83 failed: 'DebateRound' object has no attribute 'responses'
2025-06-28 20:34:42,124 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:34:49,357 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:34:49,360 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:34:49,361 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:34:49,826 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:34:49,827 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:34:49,828 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:34:49,828 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:34:49,828 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:34:50,270 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:34:50,271 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:34:50,711 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:34:50,712 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:34:50,712 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:34:52,726 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:34:52,727 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:34:52,727 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:34:52,727 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:34:54,801 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:34:54,802 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:34:54,802 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:34:54,802 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:34:56,777 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:34:56,777 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:34:56,778 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:34:56,778 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:34:59,134 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:34:59,134 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:34:59,135 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'phi3:mini', 'llama3.2:1b', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-28 20:34:59,135 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:34:59,135 - system.main - INFO - System initialized successfully
2025-06-28 20:34:59,135 - api.main - INFO - System initialized successfully!
2025-06-28 20:36:26,241 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:36:33,853 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:36:33,857 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:36:33,858 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:36:34,333 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:36:34,334 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:36:34,336 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:36:34,336 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:36:34,336 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:36:34,792 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:36:34,793 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:36:35,221 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:36:35,221 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:36:35,221 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:36:37,260 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:36:37,260 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:36:37,261 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:36:37,261 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:36:39,401 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:36:39,401 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:36:39,402 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:36:39,402 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:36:41,472 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:36:41,472 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:36:41,472 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:36:41,473 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:36:43,964 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:36:43,965 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:36:43,965 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-28 20:36:43,965 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:36:43,965 - system.main - INFO - System initialized successfully
2025-06-28 20:36:43,965 - api.main - INFO - System initialized successfully!
2025-06-28 20:36:51,186 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:36:51,190 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:36:51,191 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:36:51,653 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:36:51,654 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:36:51,655 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:36:51,655 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:36:51,655 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:36:52,101 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:36:52,102 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:36:52,538 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:36:52,538 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:36:52,538 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:36:54,568 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:36:54,568 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:36:54,568 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:36:54,569 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:36:56,676 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:36:56,677 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:36:56,677 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:36:56,677 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:36:58,683 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:36:58,684 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:36:58,684 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:36:58,684 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:37:01,020 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:37:01,021 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:37:01,021 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:3b', 'llama3.2:1b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-28 20:37:01,021 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:37:01,021 - system.main - INFO - System initialized successfully
2025-06-28 20:37:01,022 - api.main - INFO - System initialized successfully!
2025-06-28 20:37:13,832 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:37:21,200 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:37:21,203 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:37:21,204 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:37:21,692 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:37:21,694 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:37:21,696 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:37:21,697 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:37:21,697 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:37:22,173 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:37:22,175 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:37:22,653 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:37:22,655 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:37:22,655 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:37:24,740 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:37:24,741 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:37:24,741 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:37:24,741 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:37:26,965 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:37:26,965 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:37:26,966 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:37:26,966 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:37:29,091 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:37:29,091 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:37:29,091 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:37:29,092 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:37:31,592 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:37:31,593 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:37:31,593 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'gemma2:2b', 'llama3.2:3b', 'llama3.2:1b'] - PERSISTENT IN MEMORY
2025-06-28 20:37:31,593 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:37:31,593 - system.main - INFO - System initialized successfully
2025-06-28 20:37:31,593 - api.main - INFO - System initialized successfully!
2025-06-28 20:37:39,293 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:37:39,297 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:37:39,298 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:37:39,764 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:37:39,765 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:37:39,766 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:37:39,766 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:37:39,766 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:37:40,204 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:37:40,204 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:37:40,668 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:37:40,669 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:37:40,669 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:37:42,712 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:37:42,713 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:37:42,713 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:37:42,713 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:37:44,854 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:37:44,854 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:37:44,854 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:37:44,855 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:37:46,805 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:37:46,805 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:37:46,805 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:37:46,805 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:37:49,194 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:37:49,194 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:37:49,195 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:1b', 'llama3.2:3b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-28 20:37:49,195 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:37:49,195 - system.main - INFO - System initialized successfully
2025-06-28 20:37:49,195 - api.main - INFO - System initialized successfully!
2025-06-28 20:37:56,258 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:37:56,262 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:37:56,262 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:37:56,754 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:37:56,756 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:37:56,762 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:37:56,763 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:37:56,763 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:37:57,218 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:37:57,219 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:37:57,667 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:37:57,667 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:37:57,668 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:37:59,696 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:37:59,697 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:37:59,697 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:37:59,697 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:38:01,819 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:38:01,820 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:38:01,820 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:38:01,820 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:38:03,848 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:38:03,848 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:38:03,848 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:38:03,848 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:38:06,208 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:38:06,209 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:38:06,209 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'llama3.2:1b', 'gemma2:2b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-28 20:38:06,209 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:38:06,209 - system.main - INFO - System initialized successfully
2025-06-28 20:38:06,210 - api.main - INFO - System initialized successfully!
2025-06-28 20:39:23,351 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:39:23,354 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:39:23,355 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:39:23,802 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:39:23,803 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:39:23,804 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:39:23,804 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:39:23,804 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:39:24,231 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:39:24,232 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:39:24,667 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:39:24,667 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:39:24,668 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:39:26,714 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:39:26,714 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:39:26,715 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:39:26,715 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:39:28,864 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:39:28,864 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:39:28,865 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:39:28,865 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:39:30,892 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:39:30,892 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:39:30,893 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:39:30,893 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:39:33,257 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:39:33,258 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:39:33,258 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:1b', 'phi3:mini', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-28 20:39:33,258 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:39:33,258 - system.main - INFO - System initialized successfully
2025-06-28 20:39:33,258 - api.main - INFO - System initialized successfully!
2025-06-28 20:40:32,586 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:40:40,216 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:40:40,220 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:40:40,220 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:40:40,678 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:40:40,678 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:40:40,679 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:40:40,680 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:40:40,680 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:40:41,129 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:40:41,129 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:40:41,572 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:40:41,572 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:40:41,572 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:40:43,580 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:40:43,580 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:40:43,580 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:40:43,580 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:40:45,694 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:40:45,694 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:40:45,694 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:40:45,694 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:40:47,712 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:40:47,712 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:40:47,712 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:40:47,713 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:40:49,902 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:40:49,902 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:40:49,903 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:3b', 'llama3.2:1b', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-28 20:40:49,903 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:40:49,903 - system.main - INFO - System initialized successfully
2025-06-28 20:40:49,903 - api.main - INFO - System initialized successfully!
2025-06-28 20:40:49,904 - api.main - INFO - Status check - debate_system: True
2025-06-28 20:40:57,025 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:41:18,599 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:41:18,603 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:41:18,603 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:41:19,054 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:41:19,054 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:41:19,056 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:41:19,056 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:41:19,056 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:41:19,498 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:41:19,499 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:41:19,949 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:41:19,949 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:41:19,949 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:41:21,946 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:41:21,946 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:41:21,947 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:41:21,947 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:41:24,078 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:41:24,078 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:41:24,078 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:41:24,078 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:41:26,052 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:41:26,053 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:41:26,053 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:41:26,053 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:41:28,400 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:41:28,401 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:41:28,401 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'gemma2:2b', 'llama3.2:1b', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-28 20:41:28,401 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:41:28,401 - system.main - INFO - System initialized successfully
2025-06-28 20:41:28,401 - api.main - INFO - System initialized successfully!
2025-06-28 20:41:33,972 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 20:41:41,492 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:41:41,495 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 20:41:41,496 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 20:41:41,959 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:41:41,960 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 20:41:41,961 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 20:41:41,961 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 20:41:41,961 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 20:41:42,392 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:41:42,392 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 20:41:42,848 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 20:41:42,849 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 20:41:42,849 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 20:41:44,860 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:41:44,861 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 20:41:44,861 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 20:41:44,861 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 20:41:47,198 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:41:47,198 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 20:41:47,198 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 20:41:47,198 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 20:41:49,194 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:41:49,194 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 20:41:49,194 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 20:41:49,194 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 20:41:51,577 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:41:51,577 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 20:41:51,577 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'gemma2:2b', 'llama3.2:1b', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-28 20:41:51,578 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 20:41:51,578 - system.main - INFO - System initialized successfully
2025-06-28 20:41:51,578 - api.main - INFO - System initialized successfully!
2025-06-28 20:41:51,579 - api.main - INFO - Status check - debate_system: True
2025-06-28 20:41:51,833 - api.main - INFO - Status check - debate_system: True
2025-06-28 20:42:01,687 - api.main - INFO - Starting debate 4df52b05-c424-49fd-81ae-3d3d2422f4d9: should ai be regulated
2025-06-28 20:42:01,687 - system.main - INFO - Starting debate: should ai be regulated
2025-06-28 20:42:01,687 - backend.debate_workflow - INFO - Starting debate: should ai be regulated
2025-06-28 20:42:01,690 - backend.debate_workflow - INFO - Initializing debate for question: should ai be regulated
2025-06-28 20:42:01,697 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-28 20:42:01,697 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:42:01,897 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:42:02,087 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-28 20:42:05,906 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:42:08,442 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:42:08,443 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-28 20:42:12,324 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:42:12,325 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1624 chars
2025-06-28 20:42:13,102 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:42:13,102 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1980 chars
2025-06-28 20:42:13,103 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-28 20:42:13,104 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-28 20:42:13,200 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.721, reached=False
2025-06-28 20:42:13,201 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 20:42:13,257 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:42:25,883 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:42:25,883 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2508 chars
2025-06-28 20:42:25,884 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 20:42:25,884 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-28 20:42:25,884 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:42:26,066 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:42:26,236 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:42:36,655 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:42:36,656 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2763 chars
2025-06-28 20:42:41,909 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:42:41,910 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1051 chars
2025-06-28 20:42:46,713 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:42:46,714 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3474 chars
2025-06-28 20:42:46,714 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-28 20:42:46,715 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-28 20:42:46,768 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.740, reached=False
2025-06-28 20:42:46,769 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 20:42:46,821 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:42:59,175 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:42:59,175 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2381 chars
2025-06-28 20:42:59,176 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 20:42:59,176 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-28 20:42:59,177 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:42:59,353 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 20:42:59,527 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:43:09,498 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:43:09,499 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2558 chars
2025-06-28 20:43:16,068 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:43:16,068 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1732 chars
2025-06-28 20:43:18,259 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:43:18,260 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2482 chars
2025-06-28 20:43:18,260 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-28 20:43:18,261 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-28 20:43:18,308 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.788, reached=False
2025-06-28 20:43:18,309 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-28 20:43:18,310 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 20:43:31,794 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 20:43:31,795 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3167 chars
2025-06-28 20:43:31,795 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-28 20:43:31,796 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-28 20:43:31,797 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-28 20:43:31,797 - api.main - INFO - Debate 4df52b05-c424-49fd-81ae-3d3d2422f4d9 completed successfully
2025-06-28 21:09:35,931 - api.main - INFO - Starting debate 8b1b34cf-b73b-4c5f-a755-4d163992aa88: should ai be regulated
2025-06-28 21:09:35,931 - system.main - INFO - Starting debate: should ai be regulated
2025-06-28 21:09:35,931 - backend.debate_workflow - INFO - Starting debate: should ai be regulated
2025-06-28 21:09:35,933 - backend.debate_workflow - INFO - Initializing debate for question: should ai be regulated
2025-06-28 21:09:35,933 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-28 21:09:35,933 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:09:36,128 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 21:09:36,325 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:09:39,321 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:09:49,644 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:09:49,645 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2218 chars
2025-06-28 21:09:50,596 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:09:50,597 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 4426 chars
2025-06-28 21:09:50,597 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-28 21:09:50,598 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-28 21:09:50,649 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.675, reached=False
2025-06-28 21:09:50,650 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 21:09:50,692 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:10:03,369 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:10:03,369 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2607 chars
2025-06-28 21:10:03,370 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 21:10:03,370 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-28 21:10:03,370 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:10:03,548 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 21:10:03,716 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:10:13,275 - api.main - INFO - Status check - debate_system: True
2025-06-28 21:10:16,670 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:10:16,671 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3673 chars
2025-06-28 21:10:23,776 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:10:23,777 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1921 chars
2025-06-28 21:10:27,979 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:10:27,980 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3645 chars
2025-06-28 21:10:27,980 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-28 21:10:27,981 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-28 21:10:28,037 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.722, reached=False
2025-06-28 21:10:28,038 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 21:10:28,087 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:10:39,261 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:10:39,262 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 1835 chars
2025-06-28 21:10:39,262 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 21:10:39,263 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-28 21:10:39,263 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:10:39,453 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 21:10:39,640 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:10:52,734 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:10:52,735 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3513 chars
2025-06-28 21:10:58,466 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:10:58,466 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1268 chars
2025-06-28 21:11:04,357 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:11:04,357 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 4354 chars
2025-06-28 21:11:04,358 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-28 21:11:04,359 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-28 21:11:04,415 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.769, reached=False
2025-06-28 21:11:04,416 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-28 21:11:04,416 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:11:16,681 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:11:16,681 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3452 chars
2025-06-28 21:11:16,682 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-28 21:11:16,682 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-28 21:11:16,683 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-28 21:11:16,683 - api.main - INFO - Debate 8b1b34cf-b73b-4c5f-a755-4d163992aa88 completed successfully
2025-06-28 21:15:09,705 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 21:15:10,183 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 21:15:10,184 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 21:15:10,633 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 21:15:10,634 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 21:15:10,634 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 21:15:12,874 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:15:12,874 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 21:15:12,875 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 21:15:12,875 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 21:15:14,975 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:15:14,976 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 21:15:14,976 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 21:15:14,976 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 21:15:16,976 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:15:16,976 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 21:15:16,977 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 21:15:16,977 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 21:15:19,357 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:15:19,358 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 21:15:19,358 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'llama3.2:1b', 'gemma2:2b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-28 21:15:19,358 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 21:15:19,358 - system.main - INFO - System initialized successfully
2025-06-28 21:16:16,322 - api.main - INFO - Shutting down LLM Debate System...
2025-06-28 21:16:23,532 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 21:16:23,536 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 21:16:23,536 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 21:16:24,000 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 21:16:24,001 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 21:16:24,002 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 21:16:24,002 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 21:16:24,002 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 21:16:24,452 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 21:16:24,452 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 21:16:24,924 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 21:16:24,925 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 21:16:24,925 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 21:16:27,221 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:16:27,221 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 21:16:27,221 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 21:16:27,221 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 21:16:29,375 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:16:29,376 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 21:16:29,376 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 21:16:29,376 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 21:16:31,459 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:16:31,459 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 21:16:31,460 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 21:16:31,460 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 21:16:33,899 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:16:33,899 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 21:16:33,899 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:1b', 'gemma2:2b', 'phi3:mini', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-28 21:16:33,899 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 21:16:33,900 - system.main - INFO - System initialized successfully
2025-06-28 21:16:33,900 - api.main - INFO - System initialized successfully!
2025-06-28 21:16:34,395 - api.main - INFO - Status check - debate_system: True
2025-06-28 21:16:35,499 - api.main - INFO - Angular dist folder found at: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 21:16:35,504 - api.main - INFO - Angular static files will be mounted from: c:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-28 21:16:35,505 - api.main - INFO - Initializing LLM Debate System...
2025-06-28 21:16:36,104 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 21:16:36,105 - system.dynamic_config - INFO - Found 20 available models
2025-06-28 21:16:36,112 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-28 21:16:36,112 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-28 21:16:36,113 - system.main - INFO - Initializing LLM Debate System...
2025-06-28 21:16:36,703 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 21:16:36,704 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-28 21:16:37,287 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-28 21:16:37,288 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-28 21:16:37,289 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-28 21:16:39,469 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:16:39,470 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-28 21:16:39,470 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-28 21:16:39,470 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-28 21:16:41,864 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:16:41,864 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-28 21:16:41,865 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-28 21:16:41,865 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-28 21:16:44,151 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:16:44,152 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-28 21:16:44,152 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-28 21:16:44,152 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-28 21:16:44,397 - api.main - INFO - Status check - debate_system: True
2025-06-28 21:16:46,777 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:16:46,778 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-28 21:16:46,778 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:3b', 'llama3.2:1b', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-28 21:16:46,778 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-28 21:16:46,778 - system.main - INFO - System initialized successfully
2025-06-28 21:16:46,779 - api.main - INFO - System initialized successfully!
2025-06-28 21:17:04,954 - api.main - INFO - Starting debate 7003ac47-1cf3-428b-a8d1-105bfa5dfaaa: should schools be there after ai revolution
2025-06-28 21:17:04,954 - system.main - INFO - Starting debate: should schools be there after ai revolution
2025-06-28 21:17:04,955 - backend.debate_workflow - INFO - Starting debate: should schools be there after ai revolution
2025-06-28 21:17:04,957 - backend.debate_workflow - INFO - Initializing debate for question: should schools be there after ai revolution
2025-06-28 21:17:04,958 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-28 21:17:04,958 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:17:05,330 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 21:17:05,749 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-28 21:17:10,081 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:17:13,111 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:17:13,112 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-28 21:17:18,558 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:17:18,559 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1753 chars
2025-06-28 21:17:20,995 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:17:20,996 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 4391 chars
2025-06-28 21:17:20,996 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-28 21:17:20,998 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-28 21:17:21,178 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.589, reached=False
2025-06-28 21:17:21,180 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 21:17:21,328 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:17:34,821 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:17:34,822 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2620 chars
2025-06-28 21:17:34,823 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 21:17:34,824 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-28 21:17:34,824 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:17:35,188 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 21:17:35,611 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:17:48,886 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:17:48,888 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3583 chars
2025-06-28 21:17:58,939 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:17:58,940 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 3150 chars
2025-06-28 21:18:00,793 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:18:00,794 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2834 chars
2025-06-28 21:18:00,794 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-28 21:18:00,795 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-28 21:18:00,953 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.760, reached=False
2025-06-28 21:18:00,954 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-28 21:18:01,103 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:18:13,812 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:18:13,813 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2172 chars
2025-06-28 21:18:13,814 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-28 21:18:13,815 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-28 21:18:13,815 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:18:14,132 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-28 21:18:14,462 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:18:31,189 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:18:31,191 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 4501 chars
2025-06-28 21:18:39,495 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:18:39,496 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2180 chars
2025-06-28 21:18:44,126 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:18:44,127 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3823 chars
2025-06-28 21:18:44,127 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-28 21:18:44,129 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-28 21:18:44,269 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.772, reached=False
2025-06-28 21:18:44,271 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-28 21:18:44,272 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-28 21:18:59,270 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-28 21:18:59,271 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3421 chars
2025-06-28 21:18:59,272 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-28 21:18:59,273 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-28 21:18:59,273 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-28 21:18:59,274 - api.main - INFO - Debate 7003ac47-1cf3-428b-a8d1-105bfa5dfaaa completed successfully
2025-06-29 11:18:04,554 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 11:18:04,556 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 11:18:04,560 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 11:18:05,087 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 11:18:05,087 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 11:18:05,601 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 11:18:05,605 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 11:18:05,606 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 11:18:09,140 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:18:09,141 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 11:18:09,141 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 11:18:09,141 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 11:18:12,342 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:18:12,343 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 11:18:12,343 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 11:18:12,343 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 11:18:15,551 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:18:15,552 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 11:18:15,552 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 11:18:15,552 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 11:18:19,538 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:18:19,538 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 11:18:19,539 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:1b', 'llama3.2:3b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-29 11:18:19,539 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 11:18:19,539 - system.main - INFO - System initialized successfully
2025-06-29 11:18:57,473 - system.main - INFO - Starting debate: what are benefits of renewable energy
2025-06-29 11:18:57,473 - backend.debate_workflow - INFO - Starting debate: what are benefits of renewable energy
2025-06-29 11:18:57,477 - backend.debate_workflow - INFO - Initializing debate for question: what are benefits of renewable energy
2025-06-29 11:18:57,477 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-29 11:18:57,477 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 11:18:57,700 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 11:18:57,883 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-29 11:19:01,754 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:19:04,789 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:19:04,789 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-29 11:19:09,644 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:19:09,644 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1868 chars
2025-06-29 11:19:10,005 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:19:10,006 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2378 chars
2025-06-29 11:19:10,006 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-29 11:19:10,007 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-29 11:19:10,352 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.705, reached=False
2025-06-29 11:19:10,353 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 11:19:10,402 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 11:19:23,020 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:19:23,021 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2470 chars
2025-06-29 11:19:23,021 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 11:19:23,022 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-29 11:19:23,022 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 11:19:23,217 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 11:19:23,418 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 11:19:33,709 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:19:33,710 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2795 chars
2025-06-29 11:19:45,293 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:19:45,293 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 3557 chars
2025-06-29 11:19:47,802 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:19:47,802 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3719 chars
2025-06-29 11:19:47,803 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-29 11:19:47,803 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-29 11:19:47,866 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.772, reached=False
2025-06-29 11:19:47,867 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 11:19:47,918 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 11:20:00,343 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:20:00,343 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2478 chars
2025-06-29 11:20:00,344 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 11:20:00,344 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-29 11:20:00,344 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 11:20:00,550 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 11:20:00,757 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 11:20:14,086 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:20:14,086 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3526 chars
2025-06-29 11:20:23,703 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:20:23,704 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2710 chars
2025-06-29 11:20:26,113 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:20:26,114 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3230 chars
2025-06-29 11:20:26,114 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-29 11:20:26,115 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-29 11:20:26,206 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.593, reached=False
2025-06-29 11:20:26,207 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-29 11:20:26,207 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 11:20:41,364 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:20:41,365 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3880 chars
2025-06-29 11:20:41,365 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-29 11:20:41,366 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-29 11:20:41,366 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-29 11:27:34,394 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 11:27:34,398 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 11:27:34,399 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 11:27:34,891 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 11:27:34,891 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 11:27:34,892 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 11:27:34,892 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 11:27:34,893 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 11:27:35,353 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 11:27:35,354 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 11:27:35,816 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 11:27:35,817 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 11:27:35,817 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 11:27:37,223 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:27:37,223 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 11:27:37,223 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 11:27:37,224 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 11:27:39,342 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:27:39,342 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 11:27:39,343 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 11:27:39,343 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 11:27:41,422 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:27:41,422 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 11:27:41,423 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 11:27:41,423 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 11:27:43,864 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:27:43,864 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 11:27:43,864 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'llama3.2:1b', 'gemma2:2b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-29 11:27:43,865 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 11:27:43,865 - system.main - INFO - System initialized successfully
2025-06-29 11:27:43,865 - api.main - INFO - System initialized successfully!
2025-06-29 11:27:47,076 - api.main - INFO - Status check - debate_system: True
2025-06-29 11:27:47,373 - api.main - INFO - Status check - debate_system: True
2025-06-29 11:27:57,376 - api.main - INFO - Status check - debate_system: True
2025-06-29 11:28:16,046 - api.main - INFO - Starting debate 6760a978-3217-43a9-bc74-0296e237e28c: should ai be regulated by governaments
2025-06-29 11:28:16,047 - system.main - INFO - Starting debate: should ai be regulated by governaments
2025-06-29 11:28:16,047 - backend.debate_workflow - INFO - Starting debate: should ai be regulated by governaments
2025-06-29 11:28:16,049 - backend.debate_workflow - INFO - Initializing debate for question: should ai be regulated by governaments
2025-06-29 11:28:16,049 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-29 11:28:16,050 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 11:28:16,257 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 11:28:16,443 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-29 11:28:20,705 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:28:23,457 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:28:23,458 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-29 11:28:27,793 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:28:27,794 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1638 chars
2025-06-29 11:28:28,132 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:28:28,133 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1880 chars
2025-06-29 11:28:28,133 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-29 11:28:28,134 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-29 11:28:28,224 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.763, reached=False
2025-06-29 11:28:28,225 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-29 11:28:28,226 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 11:28:42,868 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 11:28:42,868 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3400 chars
2025-06-29 11:28:42,868 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-29 11:28:42,869 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 1 rounds
2025-06-29 11:28:42,869 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-29 11:28:42,869 - api.main - INFO - Debate 6760a978-3217-43a9-bc74-0296e237e28c completed successfully
2025-06-29 11:50:49,264 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 12:11:17,670 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 12:11:17,674 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 12:11:17,675 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 12:11:17,675 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 12:11:17,678 - api.main - WARNING - ngrok not found in PATH. Please install ngrok to enable public URL sharing.
2025-06-29 12:11:18,143 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:11:18,144 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 12:11:18,145 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 12:11:18,145 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 12:11:18,145 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 12:11:18,601 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:11:18,601 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 12:11:19,090 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:11:19,091 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 12:11:19,091 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 12:11:20,492 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:11:20,493 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 12:11:20,493 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 12:11:20,493 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 12:11:22,665 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:11:22,666 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 12:11:22,666 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 12:11:22,666 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 12:11:24,719 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:11:24,719 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 12:11:24,720 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 12:11:24,720 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 12:11:27,083 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:11:27,083 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 12:11:27,084 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'phi3:mini', 'gemma2:2b', 'llama3.2:1b'] - PERSISTENT IN MEMORY
2025-06-29 12:11:27,084 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 12:11:27,084 - system.main - INFO - System initialized successfully
2025-06-29 12:11:27,084 - api.main - INFO - System initialized successfully!
2025-06-29 12:12:01,197 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:16:36,062 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 12:16:46,171 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 12:16:46,174 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 12:16:46,175 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 12:16:46,176 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 12:16:46,656 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:16:46,657 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 12:16:46,658 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 12:16:46,658 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 12:16:46,658 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 12:16:47,126 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:16:47,127 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 12:16:47,603 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:16:47,604 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 12:16:47,604 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 12:16:49,035 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:16:49,035 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 12:16:49,035 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 12:16:49,036 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 12:16:51,143 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:16:51,144 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 12:16:51,144 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 12:16:51,144 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 12:16:53,583 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:16:53,584 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 12:16:53,584 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 12:16:53,584 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 12:16:56,017 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:16:56,018 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 12:16:56,018 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'phi3:mini', 'gemma2:2b', 'llama3.2:1b'] - PERSISTENT IN MEMORY
2025-06-29 12:16:56,018 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 12:16:56,018 - system.main - INFO - System initialized successfully
2025-06-29 12:16:56,018 - api.main - INFO - System initialized successfully!
2025-06-29 12:17:30,843 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:19:12,221 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 12:19:12,221 - api.main - INFO - ngrok tunnel stopped
2025-06-29 12:19:21,839 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 12:19:21,843 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 12:19:21,844 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 12:19:21,844 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 12:19:22,334 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:19:22,334 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 12:19:22,335 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 12:19:22,336 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 12:19:22,336 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 12:19:22,811 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:19:22,812 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 12:19:23,280 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:19:23,281 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 12:19:23,281 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 12:19:25,280 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:19:25,280 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 12:19:25,280 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 12:19:25,281 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 12:19:27,477 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:19:27,478 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 12:19:27,478 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 12:19:27,478 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 12:19:29,569 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:19:29,569 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 12:19:29,569 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 12:19:29,569 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 12:19:32,056 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:19:32,057 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 12:19:32,057 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'phi3:mini', 'llama3.2:1b', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-29 12:19:32,057 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 12:19:32,057 - system.main - INFO - System initialized successfully
2025-06-29 12:19:32,057 - api.main - INFO - System initialized successfully!
2025-06-29 12:19:33,772 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:19:40,197 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:19:49,882 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:20:20,905 - api.main - INFO - Starting debate ff3ca025-d78d-4947-ae92-5a5ca28987d5: should artificial intelligence be regulated
2025-06-29 12:20:20,905 - system.main - INFO - Starting debate: should artificial intelligence be regulated
2025-06-29 12:20:20,905 - backend.debate_workflow - INFO - Starting debate: should artificial intelligence be regulated
2025-06-29 12:20:20,907 - backend.debate_workflow - INFO - Initializing debate for question: should artificial intelligence be regulated
2025-06-29 12:20:20,908 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-29 12:20:20,908 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 12:20:21,137 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 12:20:21,316 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-29 12:20:25,240 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:20:27,945 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:20:27,945 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-29 12:20:34,340 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:20:34,341 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2015 chars
2025-06-29 12:20:36,313 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:20:36,313 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 4354 chars
2025-06-29 12:20:36,314 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-29 12:20:36,314 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-29 12:20:36,379 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.739, reached=False
2025-06-29 12:20:36,380 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 12:20:36,429 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 12:20:48,915 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:20:48,915 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2366 chars
2025-06-29 12:20:48,916 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 12:20:48,916 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-29 12:20:48,917 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 12:20:49,112 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 12:20:49,304 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 12:20:57,817 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:20:57,818 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2133 chars
2025-06-29 12:21:09,612 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:21:09,612 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 3771 chars
2025-06-29 12:21:10,258 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:21:10,258 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2741 chars
2025-06-29 12:21:10,259 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-29 12:21:10,259 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-29 12:21:10,340 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.829, reached=False
2025-06-29 12:21:10,341 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 12:21:10,392 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 12:21:23,868 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:21:23,869 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2691 chars
2025-06-29 12:21:23,869 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 12:21:23,870 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-29 12:21:23,870 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 12:21:24,059 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 12:21:24,262 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 12:21:34,758 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:21:34,758 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2874 chars
2025-06-29 12:21:41,230 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:21:41,231 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1483 chars
2025-06-29 12:21:44,944 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:21:44,945 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3081 chars
2025-06-29 12:21:44,945 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-29 12:21:44,946 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-29 12:21:45,019 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.762, reached=False
2025-06-29 12:21:45,020 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-29 12:21:45,020 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 12:21:59,900 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:21:59,901 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3508 chars
2025-06-29 12:21:59,901 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-29 12:21:59,902 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-29 12:21:59,902 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-29 12:21:59,902 - api.main - INFO - Debate ff3ca025-d78d-4947-ae92-5a5ca28987d5 completed successfully
2025-06-29 12:22:21,209 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:22:23,657 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:22:30,761 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:22:33,666 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:23:05,784 - api.main - INFO - Starting debate b424a041-b2e2-4c58-9fd1-352590f4e178: are schools need after ai revolution
2025-06-29 12:23:05,784 - system.main - INFO - Starting debate: are schools need after ai revolution
2025-06-29 12:23:05,784 - backend.debate_workflow - INFO - Starting debate: are schools need after ai revolution
2025-06-29 12:23:05,788 - backend.debate_workflow - INFO - Initializing debate for question: are schools need after ai revolution
2025-06-29 12:23:05,790 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-29 12:23:05,791 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 12:23:05,995 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 12:23:06,171 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 12:23:11,825 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:23:11,826 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 1588 chars
2025-06-29 12:23:18,721 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:23:18,721 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1894 chars
2025-06-29 12:23:19,040 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:23:19,041 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1536 chars
2025-06-29 12:23:19,041 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-29 12:23:19,042 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-29 12:23:19,091 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.780, reached=False
2025-06-29 12:23:19,092 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-29 12:23:19,093 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 12:23:33,402 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:23:33,403 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3254 chars
2025-06-29 12:23:33,403 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-29 12:23:33,404 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 1 rounds
2025-06-29 12:23:33,404 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-29 12:23:33,404 - api.main - INFO - Debate b424a041-b2e2-4c58-9fd1-352590f4e178 completed successfully
2025-06-29 12:24:59,680 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:25:09,369 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:29:03,656 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:30:01,210 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:30:11,360 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:30:32,810 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:30:43,368 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:30:49,259 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:30:56,450 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:30:59,357 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:33:54,224 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:34:03,016 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:34:04,359 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:35:19,905 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:35:26,200 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:35:30,366 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:37:56,341 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:38:05,883 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:38:06,362 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:38:16,008 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:38:16,355 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:38:24,296 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:38:26,359 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:38:34,362 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:38:37,599 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:38:48,357 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:38:58,370 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:39:08,368 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:39:18,355 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:39:28,357 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:39:38,356 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:39:48,355 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:39:58,368 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:40:08,355 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:40:18,365 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:40:28,368 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:40:56,332 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 12:40:56,333 - api.main - INFO - ngrok tunnel stopped
2025-06-29 12:41:57,880 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 12:41:57,883 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 12:41:57,884 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 12:41:57,885 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 12:41:58,375 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:41:58,376 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 12:41:58,377 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 12:41:58,377 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 12:41:58,377 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 12:41:58,835 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:41:58,835 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 12:41:59,327 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:41:59,329 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 12:41:59,330 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 12:42:00,750 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:42:00,750 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 12:42:00,751 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 12:42:00,751 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 12:42:02,964 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:42:02,965 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 12:42:02,965 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 12:42:02,965 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 12:42:05,014 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:42:05,014 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 12:42:05,014 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 12:42:05,014 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 12:42:07,806 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:42:07,807 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 12:42:07,807 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:1b', 'llama3.2:3b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-29 12:42:07,807 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 12:42:07,807 - system.main - INFO - System initialized successfully
2025-06-29 12:42:07,808 - api.main - INFO - System initialized successfully!
2025-06-29 12:42:36,228 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:42:45,842 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:42:46,356 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:42:54,708 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:43:05,363 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:43:15,363 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:43:20,295 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:43:21,716 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:43:32,368 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:43:42,359 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:43:52,361 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:44:02,363 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:44:12,363 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:44:22,356 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:44:31,993 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:44:42,365 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:44:52,364 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:45:02,359 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:45:12,359 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:45:22,360 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:52:00,522 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 12:52:00,522 - api.main - INFO - ngrok tunnel stopped
2025-06-29 12:52:16,012 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 12:52:16,015 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 12:52:16,016 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 12:52:16,016 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 12:52:16,515 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:52:16,516 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 12:52:16,517 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 12:52:16,518 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 12:52:16,518 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 12:52:16,973 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:52:16,976 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 12:52:17,424 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:52:17,425 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 12:52:17,425 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 12:52:18,833 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:52:18,833 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 12:52:18,834 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 12:52:18,834 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 12:52:21,317 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:52:21,318 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 12:52:21,318 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 12:52:21,318 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 12:52:23,735 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:52:23,735 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 12:52:23,736 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 12:52:23,736 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 12:52:26,111 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:52:26,111 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 12:52:26,112 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:1b', 'llama3.2:3b', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-29 12:52:26,112 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 12:52:26,112 - system.main - INFO - System initialized successfully
2025-06-29 12:52:26,112 - api.main - INFO - System initialized successfully!
2025-06-29 12:52:39,269 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:52:48,762 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:52:49,354 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:52:58,401 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:53:08,403 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:53:18,404 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:53:29,354 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:53:39,359 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:53:49,355 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:53:59,368 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:54:09,365 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:54:19,355 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:54:30,369 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:54:47,053 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:54:48,415 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:54:55,733 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:54:57,890 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:54:58,992 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:55:00,370 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:55:07,613 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:55:11,366 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:55:17,360 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:55:21,364 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:55:31,358 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:55:41,363 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:55:51,363 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:56:01,362 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:56:11,361 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:56:21,356 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:56:31,361 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:56:41,362 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:56:51,361 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:57:53,824 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 12:57:53,824 - api.main - INFO - ngrok tunnel stopped
2025-06-29 12:58:02,675 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 12:58:02,679 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 12:58:02,680 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 12:58:02,681 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 12:58:03,165 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:58:03,165 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 12:58:03,167 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 12:58:03,167 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 12:58:03,167 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 12:58:03,661 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:58:03,662 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 12:58:04,111 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 12:58:04,111 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 12:58:04,112 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 12:58:05,515 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:58:05,516 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 12:58:05,516 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 12:58:05,516 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 12:58:07,665 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:58:07,665 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 12:58:07,666 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 12:58:07,666 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 12:58:10,061 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:58:10,061 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 12:58:10,061 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 12:58:10,062 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 12:58:12,406 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 12:58:12,407 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 12:58:12,407 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:3b', 'gemma2:2b', 'llama3.2:1b'] - PERSISTENT IN MEMORY
2025-06-29 12:58:12,407 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 12:58:12,408 - system.main - INFO - System initialized successfully
2025-06-29 12:58:12,408 - api.main - INFO - System initialized successfully!
2025-06-29 12:58:51,329 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:01,356 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:01,939 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:05,550 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:06,422 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:11,588 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:12,413 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:15,819 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:17,667 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:23,675 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:26,365 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:33,368 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:36,366 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:43,671 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:46,365 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:53,368 - api.main - INFO - Status check - debate_system: True
2025-06-29 12:59:56,370 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:00:03,669 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:00:06,355 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:00:13,368 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:00:16,355 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:00:23,686 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:00:26,354 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:01:07,798 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:01:13,993 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:01:26,867 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:01:34,662 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:01:35,421 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:01:35,967 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:29:44,276 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:29:48,419 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:29:52,692 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:29:54,371 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:29:59,366 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:30:03,363 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:30:04,359 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:30:09,364 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:30:14,367 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:30:19,366 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:30:24,359 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:30:29,687 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:30:34,367 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:30:39,363 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:30:44,362 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:30:49,368 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:30:54,364 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:30:59,679 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:31:30,374 - api.main - INFO - Status check - debate_system: True
2025-06-29 13:31:30,381 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:05:19,889 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:05:30,358 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:05:40,355 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:05:50,358 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:06:00,367 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:06:10,369 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:06:20,371 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:06:22,007 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 14:06:22,008 - api.main - INFO - ngrok tunnel stopped
2025-06-29 14:06:29,789 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:06:29,793 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:06:29,794 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 14:06:29,794 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 14:06:30,293 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:06:30,293 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 14:06:30,295 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 14:06:30,295 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 14:06:30,295 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 14:06:30,769 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:06:30,771 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 14:06:31,248 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:06:31,248 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 14:06:31,248 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 14:06:32,715 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:06:32,715 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 14:06:32,716 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 14:06:32,716 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 14:06:34,956 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:06:34,956 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 14:06:34,956 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 14:06:34,957 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 14:06:37,507 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:06:37,508 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 14:06:37,508 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 14:06:37,508 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 14:06:40,043 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:06:40,043 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 14:06:40,044 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:3b', 'llama3.2:1b', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-29 14:06:40,044 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 14:06:40,044 - system.main - INFO - System initialized successfully
2025-06-29 14:06:40,044 - api.main - INFO - System initialized successfully!
2025-06-29 14:06:49,106 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:06:49,110 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:06:49,111 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 14:06:49,111 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 14:06:49,602 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:06:49,603 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 14:06:49,604 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 14:06:49,604 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 14:06:49,605 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 14:06:50,074 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:06:50,074 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 14:06:50,543 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:06:50,543 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 14:06:50,544 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 14:06:52,528 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:06:52,528 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 14:06:52,529 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 14:06:52,529 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 14:06:54,656 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:06:54,657 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 14:06:54,657 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 14:06:54,657 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 14:06:57,121 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:06:57,121 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 14:06:57,121 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 14:06:57,122 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 14:06:59,508 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:06:59,509 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 14:06:59,509 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'phi3:mini', 'llama3.2:1b', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-29 14:06:59,509 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 14:06:59,509 - system.main - INFO - System initialized successfully
2025-06-29 14:06:59,509 - api.main - INFO - System initialized successfully!
2025-06-29 14:07:00,669 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:07:05,574 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:07:10,353 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:07:15,362 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:07:20,934 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:07:30,583 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:11:42,122 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:11:45,526 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:11:48,212 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:11:55,362 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:11:58,508 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:13:27,349 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 14:13:27,349 - api.main - INFO - ngrok tunnel stopped
2025-06-29 14:13:36,078 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:13:36,082 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:13:36,082 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 14:13:36,083 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 14:13:36,581 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:13:36,581 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 14:13:36,583 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 14:13:36,583 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 14:13:36,583 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 14:13:37,050 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:13:37,051 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 14:13:37,496 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:13:37,496 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 14:13:37,497 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 14:13:38,890 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:13:38,891 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 14:13:38,891 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 14:13:38,891 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 14:13:41,084 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:13:41,084 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 14:13:41,085 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 14:13:41,085 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 14:13:43,484 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:13:43,484 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 14:13:43,484 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 14:13:43,485 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 14:13:45,813 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:13:45,813 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 14:13:45,813 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:1b', 'llama3.2:3b', 'phi3:mini', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-29 14:13:45,813 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 14:13:45,813 - system.main - INFO - System initialized successfully
2025-06-29 14:13:45,813 - api.main - INFO - System initialized successfully!
2025-06-29 14:14:06,224 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:14:15,932 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:14:38,783 - api.main - INFO - Starting debate 589e2590-2ee6-45c9-a1d7-08a6c60ecea4: should schools be required after ai revolutions
2025-06-29 14:14:38,783 - system.main - INFO - Starting debate: should schools be required after ai revolutions
2025-06-29 14:14:38,784 - backend.debate_workflow - INFO - Starting debate: should schools be required after ai revolutions
2025-06-29 14:14:38,787 - backend.debate_workflow - INFO - Initializing debate for question: should schools be required after ai revolutions
2025-06-29 14:14:38,788 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-29 14:14:38,788 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 14:14:39,118 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 14:14:39,455 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-29 14:14:44,920 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:14:44,921 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 1389 chars
2025-06-29 14:14:47,593 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:14:47,593 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-29 14:14:53,141 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:14:53,141 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2452 chars
2025-06-29 14:14:53,172 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:14:53,173 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1848 chars
2025-06-29 14:14:53,173 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-29 14:14:53,174 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-29 14:14:53,266 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.662, reached=False
2025-06-29 14:14:53,267 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-29 14:14:53,267 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 14:15:07,744 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:15:07,745 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3457 chars
2025-06-29 14:15:07,745 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-29 14:15:07,746 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 1 rounds
2025-06-29 14:15:07,746 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-29 14:15:07,746 - api.main - INFO - Debate 589e2590-2ee6-45c9-a1d7-08a6c60ecea4 completed successfully
2025-06-29 14:17:49,295 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:17:58,989 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:18:05,811 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:18:14,433 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:18:16,360 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:18:24,078 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:24:18,852 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:24:29,356 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:24:49,812 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:24:49,814 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:25:00,362 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:25:00,364 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:25:42,001 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 14:25:42,003 - api.main - INFO - ngrok tunnel stopped
2025-06-29 14:26:09,743 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:26:09,746 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:26:09,747 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 14:26:09,748 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 14:26:10,443 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:26:10,443 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 14:26:10,445 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 14:26:10,445 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 14:26:10,445 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 14:26:10,885 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:26:10,886 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 14:26:11,341 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:26:11,342 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 14:26:11,342 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 14:26:14,961 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:26:14,961 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 14:26:14,962 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 14:26:14,962 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 14:26:17,801 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:26:17,802 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 14:26:17,802 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 14:26:17,802 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 14:26:21,162 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:26:21,162 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 14:26:21,162 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 14:26:21,162 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 14:26:24,684 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:26:24,684 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 14:26:24,685 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:3b', 'llama3.2:1b', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-29 14:26:24,685 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 14:26:24,685 - system.main - INFO - System initialized successfully
2025-06-29 14:26:24,685 - api.main - INFO - System initialized successfully!
2025-06-29 14:26:26,339 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:26:36,037 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:28:09,326 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:28:18,986 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:29:02,475 - api.main - INFO - Starting debate 4d324e95-f7fd-4c2e-8f6e-6c822e32729c: should docter be there?
2025-06-29 14:29:02,475 - system.main - INFO - Starting debate: should docter be there?
2025-06-29 14:29:02,475 - backend.debate_workflow - INFO - Starting debate: should docter be there?
2025-06-29 14:29:02,482 - backend.debate_workflow - INFO - Initializing debate for question: should docter be there?
2025-06-29 14:29:02,483 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-29 14:29:02,483 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 14:29:02,689 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 14:29:02,888 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-29 14:29:06,786 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:29:08,076 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:29:08,076 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-29 14:29:11,272 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:29:11,273 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1770 chars
2025-06-29 14:29:15,236 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:29:15,237 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2350 chars
2025-06-29 14:29:15,238 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-29 14:29:15,239 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-29 14:29:15,324 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.423, reached=False
2025-06-29 14:29:15,325 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-29 14:29:15,326 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 14:29:29,986 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:29:29,987 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3201 chars
2025-06-29 14:29:29,987 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-29 14:29:29,988 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 1 rounds
2025-06-29 14:29:29,988 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-29 14:29:29,988 - api.main - INFO - Debate 4d324e95-f7fd-4c2e-8f6e-6c822e32729c completed successfully
2025-06-29 14:30:45,906 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:30:56,353 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:33:24,414 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 14:33:24,415 - api.main - INFO - ngrok tunnel stopped
2025-06-29 14:33:55,514 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:33:55,518 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:33:55,519 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 14:33:55,519 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 14:33:56,007 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:33:56,008 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 14:33:56,009 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 14:33:56,009 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 14:33:56,010 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 14:33:56,462 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:33:56,463 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 14:33:56,901 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:33:56,901 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 14:33:56,902 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 14:33:58,605 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:33:58,606 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 14:33:58,606 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 14:33:58,606 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 14:34:00,759 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:34:00,760 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 14:34:00,760 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 14:34:00,760 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 14:34:03,174 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:34:03,175 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 14:34:03,175 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 14:34:03,175 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 14:34:05,577 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:34:05,578 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 14:34:05,578 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:1b', 'phi3:mini', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-29 14:34:05,578 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 14:34:05,578 - system.main - INFO - System initialized successfully
2025-06-29 14:34:05,578 - api.main - INFO - System initialized successfully!
2025-06-29 14:34:19,255 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:34:29,378 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:37:58,638 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:38:08,353 - api.main - INFO - Status check - debate_system: True
2025-06-29 14:39:33,651 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 14:39:33,651 - api.main - INFO - ngrok tunnel stopped
2025-06-29 14:40:03,143 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:40:29,870 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:40:29,874 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:40:29,874 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 14:40:29,875 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 14:40:30,356 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:40:30,356 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 14:40:30,358 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 14:40:30,358 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 14:40:30,359 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 14:40:30,820 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:40:30,821 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 14:40:31,269 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:40:31,270 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 14:40:31,270 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 14:40:32,674 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:40:32,675 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 14:40:32,675 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 14:40:32,675 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 14:40:34,847 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:40:34,847 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 14:40:34,847 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 14:40:34,847 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 14:40:37,246 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:40:37,246 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 14:40:37,247 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 14:40:37,247 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 14:40:39,666 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:40:39,666 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 14:40:39,666 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'gemma2:2b', 'llama3.2:1b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-29 14:40:39,667 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 14:40:39,667 - system.main - INFO - System initialized successfully
2025-06-29 14:40:39,667 - api.main - INFO - System initialized successfully!
2025-06-29 14:40:51,750 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 14:40:51,751 - api.main - INFO - ngrok tunnel stopped
2025-06-29 14:41:10,072 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:41:10,076 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:41:10,077 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 14:41:10,078 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 14:41:10,563 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:41:10,564 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 14:41:10,565 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 14:41:10,565 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 14:41:10,565 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 14:41:11,015 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:41:11,016 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 14:41:11,459 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:41:11,459 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 14:41:11,460 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 14:41:13,167 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:41:13,167 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 14:41:13,167 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 14:41:13,167 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 14:41:15,340 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:41:15,340 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 14:41:15,340 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 14:41:15,340 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 14:41:17,767 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:41:17,767 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 14:41:17,768 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 14:41:17,768 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 14:41:19,162 - api.main - WARNING - Attempt 1: Could not get ngrok URL via API: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024BA3DF3B60>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-06-29 14:41:20,249 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:41:20,250 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 14:41:20,250 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:1b', 'phi3:mini', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-29 14:41:20,250 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 14:41:20,250 - system.main - INFO - System initialized successfully
2025-06-29 14:41:20,250 - api.main - INFO - System initialized successfully!
2025-06-29 14:41:33,128 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:41:33,131 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:41:33,132 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 14:41:33,132 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 14:41:33,617 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:41:33,618 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 14:41:33,619 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 14:41:33,619 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 14:41:33,619 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 14:41:34,065 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:41:34,066 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 14:41:34,518 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:41:34,518 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 14:41:34,519 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 14:41:36,265 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:41:36,265 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 14:41:36,266 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 14:41:36,266 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 14:41:38,392 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:41:38,392 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 14:41:38,393 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 14:41:38,393 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 14:41:40,831 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:41:40,832 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 14:41:40,832 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 14:41:40,832 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 14:41:43,245 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:41:43,245 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 14:41:43,246 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-29 14:41:43,246 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 14:41:43,246 - system.main - INFO - System initialized successfully
2025-06-29 14:41:43,246 - api.main - INFO - System initialized successfully!
2025-06-29 14:42:19,691 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 14:42:19,691 - api.main - INFO - ngrok tunnel stopped
2025-06-29 14:43:14,287 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:43:14,291 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:43:14,291 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 14:43:14,292 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 14:43:14,860 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:43:14,861 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 14:43:14,862 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 14:43:14,863 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 14:43:14,863 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 14:43:15,378 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:43:15,379 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 14:43:15,823 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:43:15,823 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 14:43:15,823 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 14:43:17,535 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:43:17,535 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 14:43:17,536 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 14:43:17,536 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 14:43:19,652 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:43:19,652 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 14:43:19,653 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 14:43:19,653 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 14:43:22,061 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:43:22,061 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 14:43:22,062 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 14:43:22,062 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 14:43:24,491 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:43:24,491 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 14:43:24,492 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'gemma2:2b', 'llama3.2:3b', 'llama3.2:1b'] - PERSISTENT IN MEMORY
2025-06-29 14:43:24,492 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 14:43:24,492 - system.main - INFO - System initialized successfully
2025-06-29 14:43:24,492 - api.main - INFO - System initialized successfully!
2025-06-29 14:49:35,574 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 14:49:35,575 - api.main - INFO - ngrok tunnel stopped
2025-06-29 14:49:47,992 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:49:47,995 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 14:49:47,996 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 14:49:47,997 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 14:49:48,483 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:49:48,484 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 14:49:48,486 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 14:49:48,486 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 14:49:48,486 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 14:49:48,949 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:49:48,949 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 14:49:49,415 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 14:49:49,416 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 14:49:49,416 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 14:49:50,823 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:49:50,824 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 14:49:50,824 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 14:49:50,824 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 14:49:52,934 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:49:52,935 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 14:49:52,935 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 14:49:52,935 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 14:49:55,321 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:49:55,321 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 14:49:55,321 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 14:49:55,321 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 14:49:57,662 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 14:49:57,662 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 14:49:57,662 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-29 14:49:57,663 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 14:49:57,663 - system.main - INFO - System initialized successfully
2025-06-29 14:49:57,663 - api.main - INFO - System initialized successfully!
2025-06-29 14:54:02,647 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 14:54:02,648 - api.main - INFO - ngrok tunnel stopped
2025-06-29 16:34:18,028 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 16:34:18,033 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 16:34:18,033 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 16:34:18,034 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 16:34:20,546 - backend.ollama_integration - ERROR - Failed to list models: All connection attempts failed
2025-06-29 16:34:20,546 - system.dynamic_config - INFO - Found 0 available models
2025-06-29 16:34:20,546 - api.main - ERROR - Failed to initialize system: No suitable small models found
2025-06-29 16:35:08,280 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 16:35:08,281 - api.main - INFO - ngrok tunnel stopped
2025-06-29 16:35:52,945 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 16:35:52,948 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 16:35:52,949 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 16:35:52,949 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 16:35:53,435 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 16:35:53,436 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 16:35:53,437 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 16:35:53,437 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 16:35:53,437 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 16:35:53,908 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 16:35:53,910 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 16:35:54,370 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 16:35:54,371 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 16:35:54,371 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 16:35:55,816 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:35:55,816 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 16:35:55,817 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 16:35:55,817 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 16:35:58,044 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:35:58,044 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 16:35:58,045 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 16:35:58,045 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 16:36:00,514 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:36:00,515 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 16:36:00,515 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 16:36:00,515 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 16:36:02,922 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:36:02,922 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 16:36:02,923 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'llama3.2:1b', 'gemma2:2b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-29 16:36:02,923 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 16:36:02,923 - system.main - INFO - System initialized successfully
2025-06-29 16:36:02,923 - api.main - INFO - System initialized successfully!
2025-06-29 16:37:46,988 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 16:37:46,988 - api.main - INFO - ngrok tunnel stopped
2025-06-29 16:37:56,663 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 16:37:56,667 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 16:37:56,667 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 16:37:56,668 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 16:37:57,158 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 16:37:57,159 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 16:37:57,160 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 16:37:57,160 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 16:37:57,160 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 16:37:57,615 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 16:37:57,616 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 16:37:58,082 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 16:37:58,082 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 16:37:58,082 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 16:38:00,065 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:38:00,065 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 16:38:00,065 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 16:38:00,066 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 16:38:02,251 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:38:02,251 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 16:38:02,252 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 16:38:02,252 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 16:38:04,741 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:38:04,742 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 16:38:04,742 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 16:38:04,742 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 16:38:07,195 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:38:07,196 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 16:38:07,196 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:1b', 'llama3.2:3b', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-29 16:38:07,197 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 16:38:07,197 - system.main - INFO - System initialized successfully
2025-06-29 16:38:07,197 - api.main - INFO - System initialized successfully!
2025-06-29 16:48:45,256 - api.main - INFO - Starting debate f016bc87-c486-42f8-83f4-6e6900493934: Are current Wars between countries justified?
2025-06-29 16:48:45,257 - system.main - INFO - Starting debate: Are current Wars between countries justified?
2025-06-29 16:48:45,257 - backend.debate_workflow - INFO - Starting debate: Are current Wars between countries justified?
2025-06-29 16:48:45,259 - backend.debate_workflow - INFO - Initializing debate for question: Are current Wars between countries justified?
2025-06-29 16:48:45,260 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-29 16:48:45,260 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 16:48:45,444 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 16:48:45,628 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-29 16:48:48,032 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:48:50,728 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:48:50,729 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-29 16:48:55,296 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:48:55,297 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2008 chars
2025-06-29 16:48:56,419 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:48:56,420 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2310 chars
2025-06-29 16:48:56,420 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-29 16:48:56,421 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-29 16:48:56,486 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.527, reached=False
2025-06-29 16:48:56,488 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 16:48:56,531 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 16:49:08,749 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:49:08,750 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2282 chars
2025-06-29 16:49:08,750 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 16:49:08,751 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-29 16:49:08,751 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 16:49:08,951 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 16:49:09,153 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 16:49:24,667 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:49:24,667 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 4645 chars
2025-06-29 16:49:31,483 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:49:31,484 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1914 chars
2025-06-29 16:49:37,120 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:49:37,120 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 4257 chars
2025-06-29 16:49:37,121 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-29 16:49:37,121 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-29 16:49:37,198 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.415, reached=False
2025-06-29 16:49:37,199 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 16:49:37,245 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 16:49:49,449 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:49:49,450 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2191 chars
2025-06-29 16:49:49,450 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 16:49:49,451 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-29 16:49:49,451 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 16:49:49,635 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 16:49:49,814 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 16:50:00,656 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:50:00,657 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2968 chars
2025-06-29 16:50:12,848 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:50:12,849 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 3976 chars
2025-06-29 16:50:15,862 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:50:15,863 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3962 chars
2025-06-29 16:50:15,863 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-29 16:50:15,864 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-29 16:50:15,917 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.680, reached=False
2025-06-29 16:50:15,918 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-29 16:50:15,918 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 16:50:31,111 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 16:50:31,112 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3805 chars
2025-06-29 16:50:31,112 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-29 16:50:31,113 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-29 16:50:31,113 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-29 16:50:31,113 - api.main - INFO - Debate f016bc87-c486-42f8-83f4-6e6900493934 completed successfully
2025-06-29 17:03:04,273 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 17:03:04,274 - api.main - INFO - ngrok tunnel stopped
2025-06-29 17:03:19,911 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 17:03:19,914 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 17:03:19,915 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 17:03:19,915 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 17:03:20,390 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 17:03:20,391 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 17:03:20,393 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 17:03:20,393 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 17:03:20,393 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 17:03:20,840 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 17:03:20,840 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 17:03:21,330 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 17:03:21,331 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 17:03:21,331 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 17:03:22,755 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:03:22,756 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 17:03:22,756 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 17:03:22,756 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 17:03:24,982 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:03:24,982 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 17:03:24,982 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 17:03:24,982 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 17:03:27,388 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:03:27,388 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 17:03:27,389 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 17:03:27,389 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 17:03:29,742 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:03:29,742 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 17:03:29,742 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:1b', 'gemma2:2b', 'phi3:mini', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-29 17:03:29,742 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 17:03:29,742 - system.main - INFO - System initialized successfully
2025-06-29 17:03:29,743 - api.main - INFO - System initialized successfully!
2025-06-29 17:08:07,046 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 17:08:07,047 - api.main - INFO - ngrok tunnel stopped
2025-06-29 17:08:14,942 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 17:08:14,947 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 17:08:14,948 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 17:08:14,949 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 17:08:15,473 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 17:08:15,474 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 17:08:15,476 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 17:08:15,476 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 17:08:15,476 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 17:08:15,971 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 17:08:15,972 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 17:08:16,438 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 17:08:16,438 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 17:08:16,438 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 17:08:18,233 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:08:18,233 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 17:08:18,234 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 17:08:18,234 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 17:08:20,698 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:08:20,699 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 17:08:20,699 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 17:08:20,699 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 17:08:23,162 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:08:23,162 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 17:08:23,162 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 17:08:23,162 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 17:08:23,997 - api.main - WARNING - Attempt 1: Could not get ngrok URL via API: HTTPConnectionPool(host='localhost', port=4040): Max retries exceeded with url: /api/tunnels (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000019B93157A10>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-06-29 17:08:25,637 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:08:25,638 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 17:08:25,638 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'gemma2:2b', 'llama3.2:3b', 'llama3.2:1b'] - PERSISTENT IN MEMORY
2025-06-29 17:08:25,638 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 17:08:25,638 - system.main - INFO - System initialized successfully
2025-06-29 17:08:25,638 - api.main - INFO - System initialized successfully!
2025-06-29 17:08:33,564 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 17:08:33,568 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 17:08:33,568 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 17:08:33,773 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 17:08:34,066 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 17:08:34,067 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 17:08:34,068 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 17:08:34,068 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 17:08:34,068 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 17:08:34,538 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 17:08:34,538 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 17:08:35,000 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 17:08:35,001 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 17:08:35,001 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 17:08:36,732 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:08:36,732 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 17:08:36,733 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 17:08:36,733 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 17:08:38,913 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:08:38,914 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 17:08:38,914 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 17:08:38,914 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 17:08:41,325 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:08:41,326 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 17:08:41,326 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 17:08:41,326 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 17:08:43,791 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:08:43,791 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 17:08:43,792 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:3b', 'llama3.2:1b', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-29 17:08:43,792 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 17:08:43,792 - system.main - INFO - System initialized successfully
2025-06-29 17:08:43,792 - api.main - INFO - System initialized successfully!
2025-06-29 17:09:11,327 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 17:09:11,328 - api.main - INFO - ngrok tunnel stopped
2025-06-29 17:09:21,871 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 17:09:21,875 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 17:09:21,876 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 17:09:22,069 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 17:09:22,368 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 17:09:22,369 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 17:09:22,371 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 17:09:22,371 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 17:09:22,371 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 17:09:22,822 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 17:09:22,823 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 17:09:23,278 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 17:09:23,279 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 17:09:23,279 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 17:09:25,023 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:09:25,023 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 17:09:25,024 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 17:09:25,024 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 17:09:27,449 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:09:27,449 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 17:09:27,449 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 17:09:27,450 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 17:09:29,882 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:09:29,883 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 17:09:29,883 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 17:09:29,883 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 17:09:32,328 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:09:32,328 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 17:09:32,329 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'gemma2:2b', 'llama3.2:1b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-29 17:09:32,329 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 17:09:32,329 - system.main - INFO - System initialized successfully
2025-06-29 17:09:32,329 - api.main - INFO - System initialized successfully!
2025-06-29 17:12:35,796 - api.main - INFO - Starting debate b766d64d-c9c6-4390-95ed-e57ddb1d06e9: Will AI replace human jobs in near term?
2025-06-29 17:12:35,797 - system.main - INFO - Starting debate: Will AI replace human jobs in near term?
2025-06-29 17:12:35,798 - backend.debate_workflow - INFO - Starting debate: Will AI replace human jobs in near term?
2025-06-29 17:12:35,805 - backend.debate_workflow - INFO - Initializing debate for question: Will AI replace human jobs in near term?
2025-06-29 17:12:35,808 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-29 17:12:35,808 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:12:36,011 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 17:12:36,205 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-29 17:12:40,823 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:12:40,824 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 1100 chars
2025-06-29 17:12:43,648 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:12:43,649 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-29 17:12:47,781 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:12:47,781 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1746 chars
2025-06-29 17:12:48,366 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:12:48,367 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1725 chars
2025-06-29 17:12:48,367 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-29 17:12:48,368 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-29 17:12:48,433 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.715, reached=False
2025-06-29 17:12:48,434 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 17:12:48,481 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:13:01,621 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:13:01,622 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2598 chars
2025-06-29 17:13:01,623 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 17:13:01,623 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-29 17:13:01,624 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:13:01,818 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 17:13:01,997 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:13:17,913 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:13:17,914 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 4423 chars
2025-06-29 17:13:23,410 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:13:23,410 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1225 chars
2025-06-29 17:13:27,950 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:13:27,951 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3020 chars
2025-06-29 17:13:27,951 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-29 17:13:27,952 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-29 17:13:28,007 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.735, reached=False
2025-06-29 17:13:28,009 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 17:13:28,063 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:13:41,106 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:13:41,107 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2462 chars
2025-06-29 17:13:41,107 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 17:13:41,107 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-29 17:13:41,108 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:13:41,307 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 17:13:41,484 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:13:57,398 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:13:57,399 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 4586 chars
2025-06-29 17:14:04,445 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:14:04,446 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1922 chars
2025-06-29 17:14:08,909 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:14:08,910 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3240 chars
2025-06-29 17:14:08,910 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-29 17:14:08,911 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-29 17:14:08,981 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.776, reached=False
2025-06-29 17:14:08,983 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 17:14:09,034 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:14:21,842 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:14:21,843 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2430 chars
2025-06-29 17:14:21,843 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 17:14:21,844 - backend.debate_workflow - INFO - Collecting rebuttals for round 4
2025-06-29 17:14:21,844 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:14:22,024 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 17:14:22,211 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:14:33,104 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:14:33,105 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2976 chars
2025-06-29 17:14:43,643 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:14:43,643 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 3194 chars
2025-06-29 17:14:45,547 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:14:45,547 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2828 chars
2025-06-29 17:14:45,548 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 4
2025-06-29 17:14:45,549 - backend.debate_workflow - INFO - Analyzing consensus for round 4
2025-06-29 17:14:45,601 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.712, reached=False
2025-06-29 17:14:45,603 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 17:14:45,651 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:14:58,287 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:14:58,288 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2409 chars
2025-06-29 17:14:58,288 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 17:14:58,289 - backend.debate_workflow - INFO - Collecting rebuttals for round 5
2025-06-29 17:14:58,289 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:14:58,474 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 17:14:58,655 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:15:14,707 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:15:14,708 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 4658 chars
2025-06-29 17:15:25,029 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:15:25,030 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 3288 chars
2025-06-29 17:15:26,971 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:15:26,971 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2653 chars
2025-06-29 17:15:26,972 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 5
2025-06-29 17:15:26,973 - backend.debate_workflow - INFO - Analyzing consensus for round 5
2025-06-29 17:15:27,020 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.768, reached=False
2025-06-29 17:15:27,021 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-29 17:15:27,022 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 17:15:41,950 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 17:15:41,951 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3473 chars
2025-06-29 17:15:41,951 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-29 17:15:41,952 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 5 rounds
2025-06-29 17:15:41,952 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-29 17:15:41,953 - api.main - INFO - Debate b766d64d-c9c6-4390-95ed-e57ddb1d06e9 completed successfully
2025-06-29 18:01:06,242 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 18:01:06,244 - api.main - INFO - ngrok tunnel stopped
2025-06-29 18:01:13,886 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 18:01:13,889 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 18:01:13,890 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 18:01:14,104 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 18:01:14,403 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 18:01:14,404 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 18:01:14,405 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 18:01:14,406 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 18:01:14,406 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 18:01:14,877 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 18:01:14,878 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 18:01:15,338 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 18:01:15,339 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 18:01:15,339 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 18:01:16,760 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:01:16,761 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 18:01:16,761 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 18:01:16,761 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 18:01:18,945 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:01:18,945 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 18:01:18,946 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 18:01:18,946 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 18:01:21,425 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:01:21,426 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 18:01:21,426 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 18:01:21,426 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 18:01:23,865 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:01:23,866 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 18:01:23,866 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:1b', 'phi3:mini', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-29 18:01:23,866 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 18:01:23,866 - system.main - INFO - System initialized successfully
2025-06-29 18:01:23,866 - api.main - INFO - System initialized successfully!
2025-06-29 18:03:05,874 - httpx - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-06-29 18:03:05,975 - httpx - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-06-29 18:03:06,127 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-06-29 18:03:06,493 - httpx - INFO - HTTP Request: GET https://api.gradio.app/v3/tunnel-request "HTTP/1.1 200 OK"
2025-06-29 18:03:45,266 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 18:03:45,268 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 18:03:45,269 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 18:03:45,880 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 18:03:45,881 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 18:03:46,497 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 18:03:46,498 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 18:03:46,498 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 18:03:48,403 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:03:48,404 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 18:03:48,404 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 18:03:48,405 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 18:03:50,759 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:03:50,760 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 18:03:50,760 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 18:03:50,761 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 18:03:53,392 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:03:53,393 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 18:03:53,393 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 18:03:53,393 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 18:03:55,848 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:03:55,848 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 18:03:55,848 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:3b', 'llama3.2:1b', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-29 18:03:55,848 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 18:03:55,848 - system.main - INFO - System initialized successfully
2025-06-29 18:03:55,849 - system.main - INFO - Starting debate: whats important money or happines
2025-06-29 18:03:55,849 - backend.debate_workflow - INFO - Starting debate: whats important money or happines
2025-06-29 18:03:55,851 - backend.debate_workflow - INFO - Initializing debate for question: whats important money or happines
2025-06-29 18:03:55,851 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-29 18:03:55,851 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:03:56,058 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 18:03:56,247 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-29 18:04:01,777 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:04:01,777 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 1534 chars
2025-06-29 18:04:04,557 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:04:04,557 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-29 18:04:08,260 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:04:08,260 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1424 chars
2025-06-29 18:04:08,746 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:04:08,747 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1854 chars
2025-06-29 18:04:08,747 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-29 18:04:08,748 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-29 18:04:08,821 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.819, reached=False
2025-06-29 18:04:08,822 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 18:04:08,868 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:04:22,006 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:04:22,007 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2419 chars
2025-06-29 18:04:22,007 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 18:04:22,008 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-29 18:04:22,008 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:04:22,207 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 18:04:22,398 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:04:34,896 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:04:34,897 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3603 chars
2025-06-29 18:04:42,316 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:04:42,316 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1769 chars
2025-06-29 18:04:45,596 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:04:45,597 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2878 chars
2025-06-29 18:04:45,598 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-29 18:04:45,598 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-29 18:04:45,672 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.711, reached=False
2025-06-29 18:04:45,673 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 18:04:45,722 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:04:58,647 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:04:58,648 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2463 chars
2025-06-29 18:04:58,648 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 18:04:58,649 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-29 18:04:58,649 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:04:58,867 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 18:04:59,066 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:05:15,509 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:05:15,510 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 4674 chars
2025-06-29 18:05:28,990 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:05:28,991 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 4186 chars
2025-06-29 18:05:29,035 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:05:29,035 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2736 chars
2025-06-29 18:05:29,036 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-29 18:05:29,037 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-29 18:05:29,105 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.630, reached=False
2025-06-29 18:05:29,106 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-29 18:05:29,106 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:05:42,930 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:05:42,931 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3074 chars
2025-06-29 18:05:42,931 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-29 18:05:42,932 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-29 18:05:42,932 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-29 18:10:10,741 - api.main - INFO - Starting debate b97a7579-7b4e-4e88-820c-7d06b09c95ca: whats important money or happiness
2025-06-29 18:10:10,741 - system.main - INFO - Starting debate: whats important money or happiness
2025-06-29 18:10:10,742 - backend.debate_workflow - INFO - Starting debate: whats important money or happiness
2025-06-29 18:10:10,744 - backend.debate_workflow - INFO - Initializing debate for question: whats important money or happiness
2025-06-29 18:10:10,745 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-29 18:10:10,745 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:10:10,987 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 18:10:11,163 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-29 18:10:16,179 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:10:16,180 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 1164 chars
2025-06-29 18:10:18,938 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:10:18,939 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-29 18:10:22,324 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:10:22,325 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1277 chars
2025-06-29 18:10:25,502 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:10:25,502 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 4723 chars
2025-06-29 18:10:25,503 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-29 18:10:25,503 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-29 18:10:25,578 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.772, reached=False
2025-06-29 18:10:25,580 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 18:10:25,625 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:10:38,760 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:10:38,761 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2500 chars
2025-06-29 18:10:38,761 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 18:10:38,762 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-29 18:10:38,762 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:10:38,963 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 18:10:39,145 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:10:47,898 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:10:47,899 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2347 chars
2025-06-29 18:10:56,546 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:10:56,547 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2341 chars
2025-06-29 18:10:59,112 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:10:59,112 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2794 chars
2025-06-29 18:10:59,113 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-29 18:10:59,114 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-29 18:10:59,183 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.687, reached=False
2025-06-29 18:10:59,184 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-29 18:10:59,232 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:11:12,898 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:11:12,899 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2811 chars
2025-06-29 18:11:12,899 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-29 18:11:12,900 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-29 18:11:12,900 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:11:13,107 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-29 18:11:13,301 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:11:19,700 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:11:19,701 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 1440 chars
2025-06-29 18:11:26,598 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:11:26,599 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1730 chars
2025-06-29 18:11:29,857 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:11:29,858 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2979 chars
2025-06-29 18:11:29,859 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-29 18:11:29,860 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-29 18:11:29,922 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.701, reached=False
2025-06-29 18:11:29,923 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-29 18:11:29,923 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-29 18:11:43,949 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 18:11:43,949 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3200 chars
2025-06-29 18:11:43,950 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-29 18:11:43,950 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-29 18:11:43,951 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-29 18:11:43,951 - api.main - INFO - Debate b97a7579-7b4e-4e88-820c-7d06b09c95ca completed successfully
2025-06-29 18:36:10,694 - api.main - INFO - Shutting down LLM Debate System...
2025-06-29 18:36:10,695 - api.main - INFO - ngrok tunnel stopped
2025-06-29 19:11:16,821 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 19:11:16,827 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 19:11:16,828 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 19:11:17,012 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 19:11:17,337 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 19:11:17,338 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 19:11:17,339 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 19:11:17,339 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 19:11:17,339 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 19:11:17,825 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 19:11:17,826 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 19:11:18,281 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 19:11:18,282 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 19:11:18,282 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 19:11:19,771 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 19:11:19,772 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 19:11:19,772 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 19:11:19,772 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 19:14:50,181 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 19:14:50,187 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-29 19:14:50,187 - api.main - INFO - Initializing LLM Debate System...
2025-06-29 19:14:50,379 - api.main - INFO - Starting ngrok tunnel...
2025-06-29 19:14:50,680 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 19:14:50,681 - system.dynamic_config - INFO - Found 20 available models
2025-06-29 19:14:50,681 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-29 19:14:50,681 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-29 19:14:50,681 - system.main - INFO - Initializing LLM Debate System...
2025-06-29 19:14:51,155 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 19:14:51,156 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-29 19:14:51,635 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-29 19:14:51,637 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-29 19:14:51,637 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-29 19:14:53,067 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 19:14:53,067 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-29 19:14:53,067 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-29 19:14:53,067 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-29 19:14:55,195 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 19:14:55,195 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-29 19:14:55,196 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-29 19:14:55,196 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-29 19:14:57,607 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 19:14:57,607 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-29 19:14:57,607 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-29 19:14:57,607 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-29 19:15:00,006 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-29 19:15:00,006 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-29 19:15:00,006 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:1b', 'llama3.2:3b', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-29 19:15:00,006 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-29 19:15:00,006 - system.main - INFO - System initialized successfully
2025-06-29 19:15:00,007 - api.main - INFO - System initialized successfully!
2025-06-30 09:35:42,346 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 09:35:42,353 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 09:35:42,354 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 09:35:42,633 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 09:35:43,028 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 09:35:43,029 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 09:35:43,029 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 09:35:43,029 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 09:35:43,029 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 09:35:43,506 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 09:35:43,506 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 09:35:43,990 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 09:35:43,990 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 09:35:43,991 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 09:35:47,682 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 09:35:47,682 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 09:35:47,683 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 09:35:47,683 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 09:35:50,766 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 09:35:50,766 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 09:35:50,766 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 09:35:50,766 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 09:35:53,688 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 09:35:53,688 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 09:35:53,688 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 09:35:53,688 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 09:35:57,246 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 09:35:57,246 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 09:35:57,247 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:1b', 'llama3.2:3b', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-30 09:35:57,247 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 09:35:57,247 - system.main - INFO - System initialized successfully
2025-06-30 09:35:57,247 - api.main - INFO - System initialized successfully!
2025-06-30 09:35:57,247 - api.main - INFO - Shutting down LLM Debate System...
2025-06-30 09:35:57,247 - api.main - INFO - ngrok tunnel stopped
2025-06-30 09:37:17,968 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 09:37:17,971 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 09:37:17,972 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 09:37:18,162 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 09:37:18,483 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 09:37:18,487 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 09:37:18,487 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 09:37:18,488 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 09:37:18,488 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 09:37:19,006 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 09:37:19,007 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 09:37:19,489 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 09:37:19,490 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 09:37:19,490 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 09:37:21,889 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 09:37:21,889 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 09:37:21,890 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 09:37:21,890 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 09:37:24,209 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 09:37:24,209 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 09:37:24,209 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 09:37:24,209 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 09:37:26,347 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 09:37:26,347 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 09:37:26,347 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 09:37:26,347 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 09:37:28,889 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 09:37:28,890 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 09:37:28,890 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-30 09:37:28,890 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 09:37:28,890 - system.main - INFO - System initialized successfully
2025-06-30 09:37:28,890 - api.main - INFO - System initialized successfully!
2025-06-30 09:55:18,509 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 09:55:18,513 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 09:55:18,514 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 09:55:18,696 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 09:55:19,048 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 09:55:19,051 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 09:55:19,053 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 09:55:19,053 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 09:55:19,054 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 09:55:19,551 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 09:55:19,554 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 09:55:20,048 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 09:55:20,051 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 09:55:20,051 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 09:55:21,550 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 09:55:21,551 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 09:55:21,551 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 09:55:21,551 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 09:55:23,816 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 09:55:23,817 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 09:55:23,817 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 09:55:23,817 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 09:55:25,726 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 09:55:25,727 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 09:55:25,727 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 09:55:25,727 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 09:55:28,026 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 09:55:28,026 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 09:55:28,026 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:1b', 'llama3.2:3b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-30 09:55:28,027 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 09:55:28,027 - system.main - INFO - System initialized successfully
2025-06-30 09:55:28,027 - api.main - INFO - System initialized successfully!
2025-06-30 09:55:28,372 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:05:50,713 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:05:50,717 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:05:50,717 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 10:05:50,935 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 10:05:51,218 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:05:51,218 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 10:05:51,218 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 10:05:51,219 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 10:05:51,219 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 10:05:51,693 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:05:51,693 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 10:05:52,175 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:05:52,175 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 10:05:52,175 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 10:05:53,591 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:05:53,591 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 10:05:53,591 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 10:05:53,592 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 10:05:55,605 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:05:55,605 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 10:05:55,606 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 10:05:55,606 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 10:05:57,557 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:05:57,558 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 10:05:57,558 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 10:05:57,558 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 10:05:59,930 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:05:59,931 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 10:05:59,931 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-30 10:05:59,931 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 10:05:59,931 - system.main - INFO - System initialized successfully
2025-06-30 10:05:59,931 - api.main - INFO - System initialized successfully!
2025-06-30 10:06:00,488 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:06:00,513 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:07:02,245 - api.main - INFO - Starting debate 5c736317-c328-4c15-805f-e8488c7b71b5: should artificial intelligence be regulated
2025-06-30 10:07:02,245 - system.main - INFO - Starting debate: should artificial intelligence be regulated
2025-06-30 10:07:02,245 - backend.debate_workflow - INFO - Starting debate: should artificial intelligence be regulated
2025-06-30 10:07:02,252 - backend.debate_workflow - INFO - Initializing debate for question: should artificial intelligence be regulated
2025-06-30 10:07:02,254 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-30 10:07:02,254 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:07:02,456 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 10:07:02,654 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-30 10:07:05,626 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:07:05,626 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-30 10:07:06,257 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:07:18,507 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:07:18,508 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2290 chars
2025-06-30 10:07:18,826 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:07:18,827 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1775 chars
2025-06-30 10:07:18,828 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-30 10:07:18,829 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-30 10:07:19,279 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.627, reached=False
2025-06-30 10:07:19,281 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 10:07:19,338 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:07:32,218 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:07:32,219 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2682 chars
2025-06-30 10:07:32,219 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 10:07:32,220 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-30 10:07:32,220 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:07:32,396 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 10:07:32,575 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:07:42,443 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:07:42,444 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2500 chars
2025-06-30 10:07:50,688 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:07:50,689 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2471 chars
2025-06-30 10:07:52,848 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:07:52,849 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2936 chars
2025-06-30 10:07:52,849 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-30 10:07:52,850 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-30 10:07:52,940 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.817, reached=False
2025-06-30 10:07:52,941 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 10:07:52,989 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:08:04,732 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:08:04,733 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2471 chars
2025-06-30 10:08:04,733 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 10:08:04,734 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-30 10:08:04,734 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:08:04,933 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 10:08:05,125 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:08:16,591 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:08:16,592 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3026 chars
2025-06-30 10:08:23,231 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:08:23,232 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1717 chars
2025-06-30 10:08:26,316 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:08:26,317 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3070 chars
2025-06-30 10:08:26,317 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-30 10:08:26,318 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-30 10:08:26,393 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.747, reached=False
2025-06-30 10:08:26,394 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-30 10:08:26,395 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:08:38,920 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:08:38,920 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3503 chars
2025-06-30 10:08:38,921 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-30 10:08:38,921 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-30 10:08:38,921 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-30 10:08:38,921 - api.main - INFO - Debate 5c736317-c328-4c15-805f-e8488c7b71b5 completed successfully
2025-06-30 10:08:39,017 - api.main - INFO - Shutting down LLM Debate System...
2025-06-30 10:08:39,017 - api.main - INFO - ngrok tunnel stopped
2025-06-30 10:08:46,755 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:08:46,761 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:08:46,762 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 10:08:46,960 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 10:08:47,305 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:08:47,306 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 10:08:47,306 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 10:08:47,306 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 10:08:47,306 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 10:08:47,804 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:08:47,805 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 10:08:48,303 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:08:48,304 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 10:08:48,304 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 10:08:50,095 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:08:50,095 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 10:08:50,096 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 10:08:50,096 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 10:08:52,138 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:08:52,138 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 10:08:52,139 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 10:08:52,139 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 10:08:54,142 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:08:54,143 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 10:08:54,143 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 10:08:54,143 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 10:08:56,398 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:08:56,399 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 10:08:56,399 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:1b', 'llama3.2:3b', 'phi3:mini', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-30 10:08:56,399 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 10:08:56,399 - system.main - INFO - System initialized successfully
2025-06-30 10:08:56,399 - api.main - INFO - System initialized successfully!
2025-06-30 10:08:56,421 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:08:56,424 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:08:56,427 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:08:56,430 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:08:56,435 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:08:56,438 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:08:58,733 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:08:58,738 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:09:09,049 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:09:09,052 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:09:18,239 - api.main - INFO - Starting debate 846191bd-0c5d-42d6-bdea-574af555eb39: should artifical intelligence be regulated
2025-06-30 10:09:18,239 - system.main - INFO - Starting debate: should artifical intelligence be regulated
2025-06-30 10:09:18,239 - backend.debate_workflow - INFO - Starting debate: should artifical intelligence be regulated
2025-06-30 10:09:18,241 - backend.debate_workflow - INFO - Initializing debate for question: should artifical intelligence be regulated
2025-06-30 10:09:18,242 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-30 10:09:18,242 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:09:18,471 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 10:09:18,679 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-30 10:09:22,638 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:09:25,227 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:09:25,228 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-30 10:09:30,371 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:09:30,372 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2224 chars
2025-06-30 10:09:30,454 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:09:30,454 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1815 chars
2025-06-30 10:09:30,455 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-30 10:09:30,455 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-30 10:09:30,547 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.723, reached=False
2025-06-30 10:09:30,548 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-30 10:09:30,548 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:09:44,388 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:09:44,388 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3335 chars
2025-06-30 10:09:44,388 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-30 10:09:44,389 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 1 rounds
2025-06-30 10:09:44,389 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-30 10:09:44,389 - api.main - INFO - Debate 846191bd-0c5d-42d6-bdea-574af555eb39 completed successfully
2025-06-30 10:13:39,887 - pinggy_tunnel - INFO - Stopping Pinggy tunnels...
2025-06-30 10:13:39,887 - pinggy_tunnel - INFO - Pinggy tunnels stopped
2025-06-30 10:13:47,856 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:13:47,863 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:13:47,864 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 10:13:48,053 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 10:13:48,357 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:13:48,358 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 10:13:48,358 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 10:13:48,358 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 10:13:48,359 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 10:13:48,818 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:13:48,819 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 10:13:49,274 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:13:49,275 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 10:13:49,275 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 10:13:51,245 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:13:51,246 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 10:13:51,246 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 10:13:51,246 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 10:13:53,207 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:13:53,207 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 10:13:53,207 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 10:13:53,207 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 10:13:55,130 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:13:55,131 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 10:13:55,131 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 10:13:55,131 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 10:13:57,400 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:13:57,401 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 10:13:57,401 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'llama3.2:1b', 'gemma2:2b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-30 10:13:57,401 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 10:13:57,401 - system.main - INFO - System initialized successfully
2025-06-30 10:13:57,401 - api.main - INFO - System initialized successfully!
2025-06-30 10:14:52,402 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:14:52,409 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:14:52,410 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 10:14:52,621 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 10:14:52,969 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:14:52,970 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 10:14:52,970 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 10:14:52,970 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 10:14:52,970 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 10:14:53,474 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:14:53,475 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 10:14:53,932 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:14:53,934 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 10:14:53,934 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 10:14:55,970 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:14:55,970 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 10:14:55,970 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 10:14:55,971 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 10:14:57,910 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:14:57,910 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 10:14:57,910 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 10:14:57,910 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 10:14:59,902 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:14:59,902 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 10:14:59,902 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 10:14:59,902 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 10:15:02,313 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:15:02,313 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 10:15:02,314 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'gemma2:2b', 'llama3.2:1b', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-30 10:15:02,314 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 10:15:02,314 - system.main - INFO - System initialized successfully
2025-06-30 10:15:02,314 - api.main - INFO - System initialized successfully!
2025-06-30 10:15:05,623 - api.main - INFO - Current Pinggy URLs: {}
2025-06-30 10:15:05,641 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {}}
2025-06-30 10:15:09,642 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:15:09,642 - api.main - INFO - Current Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:15:09,644 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:15:16,737 - api.main - INFO - Updated Pinggy URLs: {'backend': 'https://9.a.pinggy.io', 'frontend': 'https://1.a.pinggy.io'}
2025-06-30 10:15:19,330 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:15:19,330 - api.main - INFO - Current Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:15:19,334 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:16:20,303 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:16:20,303 - api.main - INFO - Current Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:16:20,307 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:16:30,752 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:16:30,753 - api.main - INFO - Current Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:16:30,754 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:18:01,611 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:18:01,611 - api.main - INFO - Current Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:18:01,612 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:18:03,318 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:18:03,318 - api.main - INFO - Current Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:18:03,320 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:18:04,075 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:18:04,076 - api.main - INFO - Current Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:18:04,077 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:18:14,747 - api.main - INFO - Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:18:14,747 - api.main - INFO - Current Pinggy URLs: {'backend': None, 'frontend': None}
2025-06-30 10:18:14,749 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {'backend': None, 'frontend': None}}
2025-06-30 10:18:40,689 - pinggy_tunnel - INFO - Stopping Pinggy tunnels...
2025-06-30 10:18:40,689 - pinggy_tunnel - INFO - Pinggy tunnels stopped
2025-06-30 10:18:48,693 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:18:48,701 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:18:48,701 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 10:18:48,891 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 10:18:49,196 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:18:49,197 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 10:18:49,197 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 10:18:49,197 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 10:18:49,198 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 10:18:49,666 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:18:49,667 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 10:18:50,125 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:18:50,126 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 10:18:50,126 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 10:18:52,139 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:18:52,140 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 10:18:52,140 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 10:18:52,140 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 10:18:54,091 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:18:54,091 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 10:18:54,092 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 10:18:54,092 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 10:18:55,980 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:18:55,980 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 10:18:55,980 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 10:18:55,980 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 10:18:58,158 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:18:58,159 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 10:18:58,159 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-30 10:18:58,159 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 10:18:58,159 - system.main - INFO - System initialized successfully
2025-06-30 10:18:58,159 - api.main - INFO - System initialized successfully!
2025-06-30 10:18:58,165 - api.main - INFO - Current Pinggy URLs: {}
2025-06-30 10:18:58,166 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {}}
2025-06-30 10:18:58,171 - api.main - INFO - Current Pinggy URLs: {}
2025-06-30 10:18:58,172 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {}}
2025-06-30 10:19:00,411 - api.main - INFO - Current Pinggy URLs: {}
2025-06-30 10:19:00,412 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {}}
2025-06-30 10:22:28,175 - api.main - INFO - Current Pinggy URLs: {}
2025-06-30 10:22:28,180 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {}}
2025-06-30 10:22:35,685 - pinggy_tunnel - INFO - Stopping Pinggy tunnels...
2025-06-30 10:22:35,685 - pinggy_tunnel - INFO - Pinggy tunnels stopped
2025-06-30 10:22:43,490 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:22:43,497 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:22:43,497 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 10:22:43,689 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 10:22:43,981 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:22:43,982 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 10:22:43,982 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 10:22:43,982 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 10:22:43,982 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 10:22:44,484 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:22:44,484 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 10:22:44,929 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:22:44,930 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 10:22:44,930 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 10:22:46,960 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:22:46,960 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 10:22:46,960 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 10:22:46,960 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 10:22:48,936 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:22:48,937 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 10:22:48,937 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 10:22:48,937 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 10:22:50,816 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:22:50,817 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 10:22:50,817 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 10:22:50,817 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 10:22:53,141 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:22:53,141 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 10:22:53,142 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:1b', 'phi3:mini', 'gemma2:2b', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-30 10:22:53,142 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 10:22:53,142 - system.main - INFO - System initialized successfully
2025-06-30 10:22:53,142 - api.main - INFO - System initialized successfully!
2025-06-30 10:22:53,148 - api.main - INFO - Current Pinggy URLs: {}
2025-06-30 10:22:53,150 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {}}
2025-06-30 10:26:50,452 - api.main - INFO - Current Pinggy URLs: {}
2025-06-30 10:26:50,454 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {}}
2025-06-30 10:26:50,756 - api.main - INFO - Current Pinggy URLs: {}
2025-06-30 10:26:50,757 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {}}
2025-06-30 10:26:50,762 - api.main - INFO - Current Pinggy URLs: {}
2025-06-30 10:26:50,763 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {}}
2025-06-30 10:27:00,182 - api.main - INFO - Current Pinggy URLs: {}
2025-06-30 10:27:00,185 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {}}
2025-06-30 10:27:00,751 - api.main - INFO - Current Pinggy URLs: {}
2025-06-30 10:27:00,754 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {}}
2025-06-30 10:27:00,763 - api.main - INFO - Current Pinggy URLs: {}
2025-06-30 10:27:00,765 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'pinggy_urls': {}}
2025-06-30 10:27:36,483 - pinggy_tunnel - INFO - Stopping Pinggy tunnels...
2025-06-30 10:27:36,483 - pinggy_tunnel - INFO - Pinggy tunnels stopped
2025-06-30 10:27:44,117 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:27:44,124 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:27:44,125 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 10:27:44,326 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 10:27:44,630 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:27:44,631 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 10:27:44,631 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 10:27:44,631 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 10:27:44,631 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 10:27:45,105 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:27:45,106 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 10:27:45,562 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:27:45,563 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 10:27:45,563 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 10:27:47,592 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:27:47,592 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 10:27:47,592 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 10:27:47,592 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 10:27:49,562 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:27:49,563 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 10:27:49,563 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 10:27:49,563 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 10:27:51,442 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:27:51,442 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 10:27:51,442 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 10:27:51,442 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 10:27:53,678 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:27:53,678 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 10:27:53,678 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'phi3:mini', 'llama3.2:1b', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-30 10:27:53,678 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 10:27:53,678 - system.main - INFO - System initialized successfully
2025-06-30 10:27:53,678 - api.main - INFO - System initialized successfully!
2025-06-30 10:28:00,380 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:28:00,385 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:28:00,385 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 10:28:00,600 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 10:28:00,906 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:28:00,907 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 10:28:00,907 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 10:28:00,907 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 10:28:00,907 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 10:28:01,384 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:28:01,385 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 10:28:01,868 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:28:01,869 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 10:28:01,869 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 10:28:03,944 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:28:03,944 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 10:28:03,944 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 10:28:03,945 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 10:28:05,875 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:28:05,875 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 10:28:05,875 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 10:28:05,875 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 10:28:07,755 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:28:07,755 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 10:28:07,755 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 10:28:07,755 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 10:28:09,969 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:28:09,969 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 10:28:09,969 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'phi3:mini', 'gemma2:2b', 'llama3.2:1b'] - PERSISTENT IN MEMORY
2025-06-30 10:28:09,969 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 10:28:09,969 - system.main - INFO - System initialized successfully
2025-06-30 10:28:09,969 - api.main - INFO - System initialized successfully!
2025-06-30 10:28:10,294 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:28:10,306 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:28:29,397 - api.main - INFO - Updated Cloudflare URLs: {'backend': 'https://playing-individual-developing-ed.trycloudflare.com', 'frontend': 'https://carol-consoles-choice-nitrogen.trycloudflare.com'}
2025-06-30 10:40:22,566 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:40:22,568 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:40:29,809 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:40:29,811 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:40:41,056 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:40:41,058 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:41:28,223 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:41:28,225 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:41:33,145 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:41:33,147 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:41:39,061 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:41:39,062 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:41:43,802 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:41:43,804 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:44:08,715 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:44:08,720 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 10:44:08,720 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 10:44:08,933 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 10:44:09,220 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:44:09,221 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 10:44:09,221 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 10:44:09,221 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 10:44:09,221 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 10:44:09,683 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:44:09,684 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 10:44:10,139 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 10:44:10,140 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 10:44:10,140 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 10:44:11,818 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:44:11,818 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 10:44:11,818 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 10:44:11,818 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 10:44:13,648 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:44:13,651 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:44:13,919 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:44:13,923 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:44:14,138 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:44:14,138 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 10:44:14,138 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 10:44:14,138 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 10:44:16,053 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:44:16,053 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 10:44:16,054 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 10:44:16,054 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 10:44:16,240 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:44:16,241 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:44:18,360 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:44:18,361 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 10:44:18,361 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:3b', 'llama3.2:1b', 'phi3:mini'] - PERSISTENT IN MEMORY
2025-06-30 10:44:18,361 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 10:44:18,361 - system.main - INFO - System initialized successfully
2025-06-30 10:44:18,361 - api.main - INFO - System initialized successfully!
2025-06-30 10:44:18,752 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:44:18,763 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://1211-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:44:23,378 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:44:23,385 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:44:23,772 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:44:23,776 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:44:26,773 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:44:26,784 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:44:35,841 - api.main - INFO - Updated Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 10:44:42,963 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:44:42,972 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:44:44,465 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:44:44,466 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:44:54,479 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 10:44:54,481 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 10:45:13,151 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 10:45:13,163 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 10:45:22,777 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 10:45:22,789 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 10:45:32,891 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 10:45:32,909 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 10:45:47,497 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 10:45:47,506 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 10:45:55,837 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 10:45:55,846 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 10:46:06,068 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 10:46:06,077 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 10:46:10,172 - api.main - INFO - Starting debate 28ad90e9-217e-4e40-9278-2e1fc7905a43: should artificial intelligence be debated
2025-06-30 10:46:10,172 - system.main - INFO - Starting debate: should artificial intelligence be debated
2025-06-30 10:46:10,172 - backend.debate_workflow - INFO - Starting debate: should artificial intelligence be debated
2025-06-30 10:46:10,175 - backend.debate_workflow - INFO - Initializing debate for question: should artificial intelligence be debated
2025-06-30 10:46:10,176 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-30 10:46:10,176 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:46:10,376 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 10:46:10,583 - backend.ollama_integration - INFO - Loading model tinyllama:1.1b for the FIRST TIME...
2025-06-30 10:46:14,878 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:46:17,416 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:46:17,416 - backend.ollama_integration - INFO - Successfully loaded model tinyllama:1.1b - NOW IN MEMORY
2025-06-30 10:46:21,711 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:46:21,712 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1350 chars
2025-06-30 10:46:22,358 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:46:22,359 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2435 chars
2025-06-30 10:46:22,359 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-30 10:46:22,360 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-30 10:46:22,541 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.711, reached=False
2025-06-30 10:46:22,542 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 10:46:22,607 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:46:35,470 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:46:35,470 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2339 chars
2025-06-30 10:46:35,471 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 10:46:35,471 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-30 10:46:35,471 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:46:35,657 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 10:46:35,841 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:46:46,958 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:46:46,959 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2946 chars
2025-06-30 10:47:00,222 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:47:00,223 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2537 chars
2025-06-30 10:47:00,332 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:47:00,333 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 4778 chars
2025-06-30 10:47:00,333 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-30 10:47:00,334 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-30 10:47:00,416 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.744, reached=False
2025-06-30 10:47:00,417 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-30 10:47:00,417 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:47:15,077 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:47:15,077 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3564 chars
2025-06-30 10:47:15,077 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-30 10:47:15,078 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 2 rounds
2025-06-30 10:47:15,078 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-30 10:47:15,078 - api.main - INFO - Debate 28ad90e9-217e-4e40-9278-2e1fc7905a43 completed successfully
2025-06-30 10:47:26,203 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 10:47:26,218 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 10:47:36,258 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 10:47:36,270 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 10:47:52,813 - api.main - INFO - Starting debate e26bdf4d-f087-49fd-8893-28e3bc53aad8: Should artificial intelligence be regulated
2025-06-30 10:47:52,813 - system.main - INFO - Starting debate: Should artificial intelligence be regulated
2025-06-30 10:47:52,813 - backend.debate_workflow - INFO - Starting debate: Should artificial intelligence be regulated
2025-06-30 10:47:52,814 - backend.debate_workflow - INFO - Initializing debate for question: Should artificial intelligence be regulated
2025-06-30 10:47:52,815 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-30 10:47:52,815 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:47:52,996 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 10:47:53,193 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:47:56,688 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:48:02,817 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:48:02,817 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1729 chars
2025-06-30 10:48:04,649 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:48:04,649 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2358 chars
2025-06-30 10:48:04,650 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-30 10:48:04,650 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-30 10:48:04,739 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.766, reached=False
2025-06-30 10:48:04,740 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 10:48:04,809 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:48:16,968 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:48:16,968 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2235 chars
2025-06-30 10:48:16,968 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 10:48:16,969 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-30 10:48:16,969 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:48:17,144 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 10:48:17,323 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:48:20,450 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:48:28,112 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:48:28,113 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2126 chars
2025-06-30 10:48:31,003 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:48:31,004 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3015 chars
2025-06-30 10:48:31,004 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-30 10:48:31,005 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-30 10:48:31,054 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.833, reached=False
2025-06-30 10:48:31,056 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-30 10:48:31,056 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:48:41,020 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:48:41,021 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3114 chars
2025-06-30 10:48:41,021 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-30 10:48:41,021 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 2 rounds
2025-06-30 10:48:41,022 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-30 10:48:41,022 - api.main - INFO - Debate e26bdf4d-f087-49fd-8893-28e3bc53aad8 completed successfully
2025-06-30 10:53:18,030 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 10:53:18,041 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 10:53:28,093 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 10:53:28,103 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 10:53:46,170 - api.main - INFO - Starting debate 8e965d5d-26db-418c-b009-e23febfc590d: should schools be removed after ai regulation?
2025-06-30 10:53:46,171 - system.main - INFO - Starting debate: should schools be removed after ai regulation?
2025-06-30 10:53:46,171 - backend.debate_workflow - INFO - Starting debate: should schools be removed after ai regulation?
2025-06-30 10:53:46,177 - backend.debate_workflow - INFO - Initializing debate for question: should schools be removed after ai regulation?
2025-06-30 10:53:46,179 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-30 10:53:46,179 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:53:46,405 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 10:53:46,601 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:53:49,522 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:53:56,753 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:53:56,753 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2086 chars
2025-06-30 10:53:57,263 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:53:57,263 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1693 chars
2025-06-30 10:53:57,263 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-30 10:53:57,264 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-30 10:53:57,334 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.674, reached=False
2025-06-30 10:53:57,335 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 10:53:57,381 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:54:11,069 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:54:11,069 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2898 chars
2025-06-30 10:54:11,069 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 10:54:11,070 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-30 10:54:11,070 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:54:11,271 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 10:54:11,471 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:54:26,616 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:54:26,617 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 4464 chars
2025-06-30 10:54:32,552 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:54:32,553 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1420 chars
2025-06-30 10:54:36,160 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:54:36,161 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2935 chars
2025-06-30 10:54:36,161 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-30 10:54:36,162 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-30 10:54:36,226 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.683, reached=False
2025-06-30 10:54:36,227 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 10:54:36,273 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:54:48,762 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:54:48,762 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2260 chars
2025-06-30 10:54:48,763 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 10:54:48,763 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-30 10:54:48,763 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:54:48,955 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 10:54:49,136 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:54:58,809 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:54:58,809 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2544 chars
2025-06-30 10:55:09,640 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:55:09,640 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 3540 chars
2025-06-30 10:55:12,119 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:55:12,120 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3387 chars
2025-06-30 10:55:12,120 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-30 10:55:12,120 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-30 10:55:12,197 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.733, reached=False
2025-06-30 10:55:12,198 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-30 10:55:12,198 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 10:55:25,735 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 10:55:25,736 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2937 chars
2025-06-30 10:55:25,736 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-30 10:55:25,736 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-30 10:55:25,736 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-30 10:55:25,736 - api.main - INFO - Debate 8e965d5d-26db-418c-b009-e23febfc590d completed successfully
2025-06-30 11:00:13,513 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:00:13,530 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:00:23,380 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:00:23,394 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:00:47,011 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:00:47,020 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:00:56,977 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:00:56,987 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:01:00,864 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:01:00,874 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:01:13,515 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:01:13,530 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:01:23,147 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:01:23,157 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:01:23,539 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:01:23,549 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:01:33,168 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:01:33,178 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:01:37,855 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:01:37,867 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:01:39,861 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:01:39,874 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:02:26,978 - api.main - INFO - Starting debate 94f40d1b-050a-4541-873f-5fd4e2fdd270: should artificial intelligence be regulated?
2025-06-30 11:02:26,978 - system.main - INFO - Starting debate: should artificial intelligence be regulated?
2025-06-30 11:02:26,978 - backend.debate_workflow - INFO - Starting debate: should artificial intelligence be regulated?
2025-06-30 11:02:26,981 - backend.debate_workflow - INFO - Initializing debate for question: should artificial intelligence be regulated?
2025-06-30 11:02:26,984 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-30 11:02:26,986 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:02:27,198 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:02:27,400 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:02:29,814 - api.main - INFO - Starting debate 37cd9a93-dd3b-4205-9c32-872fddfbcc22: should schools exist after ai revolution
2025-06-30 11:02:29,814 - system.main - INFO - Starting debate: should schools exist after ai revolution
2025-06-30 11:02:29,814 - backend.debate_workflow - INFO - Starting debate: should schools exist after ai revolution
2025-06-30 11:02:29,815 - backend.debate_workflow - INFO - Initializing debate for question: should schools exist after ai revolution
2025-06-30 11:02:29,816 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-30 11:02:29,816 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:02:30,019 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:02:30,225 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:02:30,996 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:02:32,705 - api.main - INFO - Starting debate 1c3479e0-cd6b-4d42-8ad7-c078f8a19e4a: how important is environment?
2025-06-30 11:02:32,706 - system.main - INFO - Starting debate: how important is environment?
2025-06-30 11:02:32,706 - backend.debate_workflow - INFO - Starting debate: how important is environment?
2025-06-30 11:02:32,707 - backend.debate_workflow - INFO - Initializing debate for question: how important is environment?
2025-06-30 11:02:32,707 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-30 11:02:32,708 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:02:32,922 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:02:33,125 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:02:36,155 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:02:36,156 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1833 chars
2025-06-30 11:02:39,467 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:02:45,337 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:02:45,338 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2083 chars
2025-06-30 11:02:49,277 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:02:56,646 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:02:56,647 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1857 chars
2025-06-30 11:02:57,438 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:02:57,439 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1664 chars
2025-06-30 11:02:57,439 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-30 11:02:57,440 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-30 11:02:57,597 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.720, reached=False
2025-06-30 11:02:57,598 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 11:02:57,743 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:02:58,184 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:02:58,185 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2013 chars
2025-06-30 11:02:58,185 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-30 11:02:58,186 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-30 11:02:58,364 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.734, reached=False
2025-06-30 11:02:58,366 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 11:02:58,534 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:03:00,155 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:03:00,156 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2605 chars
2025-06-30 11:03:00,156 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-30 11:03:00,157 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-30 11:03:00,309 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.749, reached=False
2025-06-30 11:03:00,310 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 11:03:00,457 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:03:07,529 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:03:07,530 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2575 chars
2025-06-30 11:03:07,530 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 11:03:07,531 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-30 11:03:07,531 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:03:07,857 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:03:08,192 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:03:08,527 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:03:08,528 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2571 chars
2025-06-30 11:03:08,529 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 11:03:08,530 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-30 11:03:08,531 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:03:08,867 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:03:09,193 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:03:14,732 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:03:14,733 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2661 chars
2025-06-30 11:03:14,733 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 11:03:14,733 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-30 11:03:14,733 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:03:15,059 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:03:15,400 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:03:24,908 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:03:24,909 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2837 chars
2025-06-30 11:03:33,840 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:03:33,841 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1926 chars
2025-06-30 11:03:40,395 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:03:40,396 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 3282 chars
2025-06-30 11:03:46,171 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:03:46,172 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3247 chars
2025-06-30 11:03:46,893 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:03:46,894 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 4560 chars
2025-06-30 11:03:46,895 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-30 11:03:46,898 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-30 11:03:47,051 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.704, reached=False
2025-06-30 11:03:47,054 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 11:03:47,192 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:03:49,268 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:03:49,268 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 4365 chars
2025-06-30 11:03:56,782 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:03:56,783 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 4220 chars
2025-06-30 11:04:08,435 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:04:08,436 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3156 chars
2025-06-30 11:04:08,436 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-30 11:04:08,437 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-30 11:04:08,582 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.722, reached=False
2025-06-30 11:04:08,583 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 11:04:08,760 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:04:16,007 - backend.ollama_integration - ERROR - Error calling Creative_Debater: 
2025-06-30 11:04:16,008 - backend.agents - ERROR - Error generating rebuttal for Creative_Debater: 
2025-06-30 11:04:16,008 - backend.debate_workflow - ERROR - Error collecting rebuttals: 
2025-06-30 11:04:16,009 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-30 11:04:16,143 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.749, reached=False
2025-06-30 11:04:16,144 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 11:04:16,272 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:04:24,939 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:04:24,940 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2415 chars
2025-06-30 11:04:24,940 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 11:04:24,941 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-30 11:04:24,941 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:04:25,268 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:04:25,598 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:04:25,934 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:04:25,950 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2605 chars
2025-06-30 11:04:25,952 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 11:04:25,953 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-30 11:04:25,953 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:04:26,284 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:04:26,600 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:04:30,712 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:04:30,713 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2291 chars
2025-06-30 11:04:30,713 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 11:04:30,714 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-30 11:04:30,714 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:04:31,044 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:04:31,386 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:04:46,898 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:04:46,899 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 4525 chars
2025-06-30 11:04:54,591 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:04:54,592 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1412 chars
2025-06-30 11:04:56,204 - api.main - INFO - Starting debate 782b0496-3e6f-4b9e-9439-1d015c4ff40b: Should environmental impact considerered
2025-06-30 11:04:56,204 - system.main - INFO - Starting debate: Should environmental impact considerered
2025-06-30 11:04:56,205 - backend.debate_workflow - INFO - Starting debate: Should environmental impact considerered
2025-06-30 11:04:56,206 - backend.debate_workflow - INFO - Initializing debate for question: Should environmental impact considerered
2025-06-30 11:04:56,206 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-30 11:04:56,207 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:04:56,536 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:04:56,865 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:05:01,874 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:05:01,875 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1733 chars
2025-06-30 11:05:04,792 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:05:04,793 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 4331 chars
2025-06-30 11:05:07,305 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:05:07,306 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3695 chars
2025-06-30 11:05:10,167 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:05:10,168 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 4622 chars
2025-06-30 11:05:10,168 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-30 11:05:10,169 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-30 11:05:10,313 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.527, reached=False
2025-06-30 11:05:10,316 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-30 11:05:10,316 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:05:17,196 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:05:17,197 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 4165 chars
2025-06-30 11:05:27,221 - backend.ollama_integration - ERROR - Error calling Creative_Debater: 
2025-06-30 11:05:27,221 - backend.agents - ERROR - Error generating rebuttal for Creative_Debater: 
2025-06-30 11:05:27,222 - backend.debate_workflow - ERROR - Error collecting rebuttals: 
2025-06-30 11:05:27,223 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-30 11:05:27,366 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.704, reached=False
2025-06-30 11:05:27,368 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-30 11:05:27,368 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:05:31,988 - backend.ollama_integration - ERROR - Error calling Creative_Debater: 
2025-06-30 11:05:31,988 - backend.agents - ERROR - Error generating rebuttal for Creative_Debater: 
2025-06-30 11:05:31,988 - backend.debate_workflow - ERROR - Error collecting rebuttals: 
2025-06-30 11:05:31,989 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-30 11:05:32,134 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.749, reached=False
2025-06-30 11:05:32,136 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-30 11:05:32,136 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:05:35,179 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:05:35,180 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 1227 chars
2025-06-30 11:05:39,261 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:05:39,262 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 2129 chars
2025-06-30 11:05:46,492 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:05:46,493 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3113 chars
2025-06-30 11:05:46,493 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-30 11:05:46,494 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-30 11:05:46,494 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-30 11:05:46,494 - api.main - INFO - Debate 1c3479e0-cd6b-4d42-8ad7-c078f8a19e4a completed successfully
2025-06-30 11:05:47,400 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:05:47,400 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3433 chars
2025-06-30 11:05:47,401 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-30 11:05:47,402 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-30 11:05:47,402 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-30 11:05:47,402 - api.main - INFO - Debate 94f40d1b-050a-4541-873f-5fd4e2fdd270 completed successfully
2025-06-30 11:05:54,181 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:05:54,182 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3527 chars
2025-06-30 11:05:54,182 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-30 11:05:54,183 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-30 11:05:54,183 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-30 11:05:54,183 - api.main - INFO - Debate 37cd9a93-dd3b-4205-9c32-872fddfbcc22 completed successfully
2025-06-30 11:05:57,468 - backend.ollama_integration - ERROR - Error calling Analytical_Debater: 
2025-06-30 11:05:57,468 - backend.agents - ERROR - Error generating initial response for Analytical_Debater: 
2025-06-30 11:05:57,469 - backend.debate_workflow - ERROR - Error collecting initial responses: 
2025-06-30 11:05:57,469 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-30 11:05:57,469 - backend.debate_workflow - WARNING - No debater responses to analyze
2025-06-30 11:05:57,470 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 11:05:57,471 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:06:04,130 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:06:04,130 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2145 chars
2025-06-30 11:06:04,131 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 11:06:04,131 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-30 11:06:04,132 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:06:04,469 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:06:04,794 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:06:12,580 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:06:12,580 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2099 chars
2025-06-30 11:06:18,598 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:06:18,599 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1673 chars
2025-06-30 11:06:22,348 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:06:22,349 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3120 chars
2025-06-30 11:06:22,349 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-30 11:06:22,350 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-30 11:06:22,492 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.359, reached=False
2025-06-30 11:06:22,494 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 11:06:22,625 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:06:35,500 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:06:35,500 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2635 chars
2025-06-30 11:06:35,501 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 11:06:35,501 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-30 11:06:35,501 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:06:35,856 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:06:36,203 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:06:51,034 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:06:51,035 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 4678 chars
2025-06-30 11:06:56,002 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:06:56,002 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1058 chars
2025-06-30 11:06:59,531 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:06:59,532 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2755 chars
2025-06-30 11:06:59,532 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-30 11:06:59,533 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-30 11:06:59,678 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.420, reached=False
2025-06-30 11:06:59,680 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-30 11:06:59,680 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:07:14,544 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:07:14,545 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 4140 chars
2025-06-30 11:07:14,545 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-30 11:07:14,546 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-30 11:07:14,546 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-30 11:07:14,546 - api.main - INFO - Debate 782b0496-3e6f-4b9e-9439-1d015c4ff40b completed successfully
2025-06-30 11:08:18,604 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:08:18,623 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:08:28,857 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:08:28,880 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:20:13,333 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:20:13,361 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:20:38,378 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:20:38,404 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:20:48,393 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:20:48,419 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:22:24,670 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:22:24,681 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:23:50,775 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:23:50,786 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:24:00,756 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:24:00,766 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:24:59,685 - api.main - INFO - Starting debate 7c33b5ed-14c0-4340-8d4c-76d1bb5d0284: Does God exist?
2025-06-30 11:24:59,685 - system.main - INFO - Starting debate: Does God exist?
2025-06-30 11:24:59,685 - backend.debate_workflow - INFO - Starting debate: Does God exist?
2025-06-30 11:24:59,699 - backend.debate_workflow - INFO - Initializing debate for question: Does God exist?
2025-06-30 11:24:59,700 - backend.debate_workflow - INFO - Collecting initial responses from debaters
2025-06-30 11:24:59,700 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:24:59,902 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:25:00,102 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:25:07,216 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:25:07,217 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 2312 chars
2025-06-30 11:25:13,765 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:25:13,765 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1914 chars
2025-06-30 11:25:14,870 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:25:14,871 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 1683 chars
2025-06-30 11:25:14,871 - backend.debate_workflow - INFO - Collected 3 initial responses
2025-06-30 11:25:14,871 - backend.debate_workflow - INFO - Analyzing consensus for round 1
2025-06-30 11:25:14,935 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.513, reached=False
2025-06-30 11:25:14,936 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 11:25:14,992 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:25:21,243 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:25:21,244 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:25:23,026 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:25:23,028 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:25:28,586 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:25:28,586 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2890 chars
2025-06-30 11:25:28,587 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 11:25:28,587 - backend.debate_workflow - INFO - Collecting rebuttals for round 2
2025-06-30 11:25:28,587 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:25:28,768 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:25:28,951 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:25:37,289 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:25:37,299 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:25:41,158 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:25:41,159 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3340 chars
2025-06-30 11:25:47,827 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:25:47,851 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:25:48,349 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:25:48,349 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1819 chars
2025-06-30 11:25:48,882 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:25:48,887 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:25:52,009 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:25:52,010 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 2999 chars
2025-06-30 11:25:52,010 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 2
2025-06-30 11:25:52,011 - backend.debate_workflow - INFO - Analyzing consensus for round 2
2025-06-30 11:25:52,164 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.594, reached=False
2025-06-30 11:25:52,166 - backend.debate_workflow - INFO - Generating orchestrator feedback
2025-06-30 11:25:52,315 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:25:58,750 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:25:58,754 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:26:01,117 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:26:01,117 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 2447 chars
2025-06-30 11:26:01,118 - backend.debate_workflow - INFO - Orchestrator feedback generated successfully
2025-06-30 11:26:01,118 - backend.debate_workflow - INFO - Collecting rebuttals for round 3
2025-06-30 11:26:01,118 - backend.ollama_integration - INFO - Model gemma2:2b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:26:01,342 - backend.ollama_integration - INFO - Model phi3:mini ALREADY LOADED (persistent) - no action needed
2025-06-30 11:26:01,528 - backend.ollama_integration - INFO - Model tinyllama:1.1b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:26:12,680 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:26:12,681 - backend.ollama_integration - WARNING - Response too long from Creative_Debater: 3042 chars
2025-06-30 11:26:18,308 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:26:18,309 - backend.ollama_integration - WARNING - Response too long from Practical_Debater: 1007 chars
2025-06-30 11:26:23,740 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:26:23,741 - backend.ollama_integration - WARNING - Response too long from Analytical_Debater: 3535 chars
2025-06-30 11:26:23,741 - backend.debate_workflow - INFO - Collected 3 rebuttal responses for round 3
2025-06-30 11:26:23,742 - backend.debate_workflow - INFO - Analyzing consensus for round 3
2025-06-30 11:26:23,795 - backend.debate_workflow - INFO - Consensus analysis: similarity=0.758, reached=False
2025-06-30 11:26:23,796 - backend.debate_workflow - INFO - Handling max rounds reached
2025-06-30 11:26:23,796 - backend.ollama_integration - INFO - Model llama3.2:3b ALREADY LOADED (persistent) - no action needed
2025-06-30 11:26:38,604 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:26:38,604 - backend.ollama_integration - WARNING - Response too long from Orchestrator: 3561 chars
2025-06-30 11:26:38,604 - backend.debate_workflow - INFO - Max rounds handling completed
2025-06-30 11:26:38,605 - backend.debate_workflow - INFO - Debate completed: DebateStatus.MAX_ROUNDS_EXCEEDED in 3 rounds
2025-06-30 11:26:38,605 - system.main - INFO - Debate completed with status: DebateStatus.MAX_ROUNDS_EXCEEDED
2025-06-30 11:26:38,605 - api.main - INFO - Debate 7c33b5ed-14c0-4340-8d4c-76d1bb5d0284 completed successfully
2025-06-30 11:27:04,348 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}
2025-06-30 11:27:04,357 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://integrating-daddy-scenes-functions.trycloudflare.com', 'frontend': 'https://associates-essay-selected-f.trycloudflare.com'}}
2025-06-30 11:27:22,212 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:27:22,213 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:27:22,218 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:27:22,220 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:27:22,225 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:27:22,227 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:27:32,756 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:27:32,757 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:27:32,761 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:27:32,763 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:27:32,767 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:27:32,769 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:27:36,404 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:27:36,406 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:27:36,533 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:27:36,535 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:27:36,569 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:27:36,570 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:27:42,091 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:27:42,094 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:27:42,101 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:27:42,104 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:27:42,112 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:27:42,114 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:28:00,802 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:28:00,803 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:28:05,818 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:28:05,821 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:28:06,047 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:28:06,049 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:28:07,138 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:28:07,140 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:28:12,770 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:28:12,775 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:29:34,070 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 11:29:34,075 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 11:29:34,075 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 11:29:34,290 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 11:29:34,572 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 11:29:34,573 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 11:29:34,573 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 11:29:34,573 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 11:29:34,573 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 11:29:35,051 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 11:29:35,051 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 11:29:35,515 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 11:29:35,516 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 11:29:35,516 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 11:29:37,563 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:29:37,563 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 11:29:37,563 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 11:29:37,563 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 11:29:39,599 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:29:39,599 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 11:29:39,599 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 11:29:39,599 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 11:29:41,896 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:29:41,897 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 11:29:41,897 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 11:29:41,897 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 11:29:44,137 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:29:44,137 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 11:29:44,137 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'phi3:mini', 'llama3.2:3b', 'llama3.2:1b'] - PERSISTENT IN MEMORY
2025-06-30 11:29:44,137 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 11:29:44,137 - system.main - INFO - System initialized successfully
2025-06-30 11:29:44,138 - api.main - INFO - System initialized successfully!
2025-06-30 11:29:44,275 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:29:44,285 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://535b-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:29:51,916 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:29:51,919 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:29:52,824 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:29:52,828 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:29:54,116 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:29:54,118 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:29:56,157 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:29:56,159 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:29:56,745 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:29:56,746 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:29:57,749 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:29:57,751 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:29:59,780 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:29:59,792 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:30:01,359 - api.main - INFO - Updated Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:30:01,743 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:30:01,744 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:30:24,806 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:30:24,808 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:30:31,061 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:30:31,062 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:32:22,236 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:32:22,250 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:32:27,814 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:32:27,827 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:32:36,865 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:32:36,890 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:32:41,889 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:32:41,913 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:33:38,660 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:33:38,667 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:33:43,323 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:33:43,325 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:24,801 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:24,802 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:31,074 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:31,082 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:49,994 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:34:50,008 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:34:52,844 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:52,847 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:52,901 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:52,904 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:53,198 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:53,200 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:53,204 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:53,206 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:53,210 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:53,212 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:53,217 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:53,219 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:55,022 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:34:55,040 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:34:58,744 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:58,746 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:58,752 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:58,754 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:58,758 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:58,760 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:58,764 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:58,767 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:59,061 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:59,062 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:34:59,070 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:34:59,072 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://449c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:35:13,008 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:35:13,025 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:35:13,061 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:35:13,077 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:35:13,109 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:35:13,125 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:35:13,151 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:35:13,163 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:35:13,187 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:35:13,199 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:35:13,224 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:35:13,236 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:35:17,819 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:35:17,828 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:35:17,849 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:35:17,858 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:35:18,827 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:35:18,837 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:35:18,859 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:35:18,868 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:35:18,887 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:35:18,896 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:35:18,915 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:35:18,924 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:32,256 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:32,268 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:32,294 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:32,304 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:32,352 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:32,363 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:32,388 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:32,399 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:32,422 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:32,434 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:32,460 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:32,475 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:37,763 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:37,774 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:37,795 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:37,803 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:37,823 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:37,833 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:37,852 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:37,862 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:37,881 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:37,891 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:37,908 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:37,917 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:43,476 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:43,487 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:43,512 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:43,523 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:43,549 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:43,561 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:43,585 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:43,598 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:43,621 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:43,632 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:43,655 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:43,666 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:48,811 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:48,821 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:48,843 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:48,852 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:48,872 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:48,881 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:48,899 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:48,908 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:48,927 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:48,937 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:38:48,956 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:38:48,965 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:43:10,169 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:43:10,179 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:43:13,748 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}
2025-06-30 11:43:13,758 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://memorial-selecting-ipod-estimation.trycloudflare.com', 'frontend': 'https://spas-pray-nail-newman.trycloudflare.com'}}
2025-06-30 11:48:16,200 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 11:48:16,207 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 11:48:16,207 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 11:48:16,442 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 11:48:16,735 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 11:48:16,736 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 11:48:16,736 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 11:48:16,736 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 11:48:16,736 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 11:48:17,233 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 11:48:17,233 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 11:48:17,683 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 11:48:17,684 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 11:48:17,684 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 11:48:19,403 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:48:19,403 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 11:48:19,403 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 11:48:19,403 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 11:48:21,571 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:48:21,572 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 11:48:21,572 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 11:48:21,572 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 11:48:23,849 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:48:23,849 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 11:48:23,850 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 11:48:23,850 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 11:48:26,228 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:48:26,229 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 11:48:26,229 - backend.ollama_integration - INFO - Currently loaded models: ['phi3:mini', 'gemma2:2b', 'llama3.2:3b', 'llama3.2:1b'] - PERSISTENT IN MEMORY
2025-06-30 11:48:26,229 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 11:48:26,229 - system.main - INFO - System initialized successfully
2025-06-30 11:48:26,229 - api.main - INFO - System initialized successfully!
2025-06-30 11:48:26,337 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:48:26,349 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://55b0-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:48:45,432 - api.main - INFO - Updated Cloudflare URLs: {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}
2025-06-30 11:49:02,334 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}
2025-06-30 11:49:02,344 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}}
2025-06-30 11:49:07,853 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}
2025-06-30 11:49:07,862 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}}
2025-06-30 11:50:52,018 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}
2025-06-30 11:50:52,032 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}}
2025-06-30 11:50:52,060 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}
2025-06-30 11:50:52,071 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}}
2025-06-30 11:50:56,763 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}
2025-06-30 11:50:56,774 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}}
2025-06-30 11:50:56,802 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}
2025-06-30 11:50:56,813 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}}
2025-06-30 11:51:48,778 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}
2025-06-30 11:51:48,794 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}}
2025-06-30 11:51:48,826 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}
2025-06-30 11:51:48,839 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}}
2025-06-30 11:51:54,771 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}
2025-06-30 11:51:54,783 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}}
2025-06-30 11:51:54,811 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}
2025-06-30 11:51:54,824 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}}
2025-06-30 11:52:13,838 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}
2025-06-30 11:52:13,857 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}}
2025-06-30 11:52:20,138 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}
2025-06-30 11:52:20,153 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://cottages-ya-entire-ti.trycloudflare.com', 'frontend': 'https://crossing-somehow-productivity-blood.trycloudflare.com'}}
2025-06-30 11:55:14,472 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 11:55:14,477 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 11:55:14,477 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 11:55:14,693 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 11:55:14,977 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 11:55:14,977 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 11:55:14,978 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 11:55:14,978 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 11:55:14,978 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 11:55:15,447 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 11:55:15,447 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 11:55:15,922 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 11:55:15,923 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 11:55:15,923 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 11:55:17,667 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:55:17,668 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 11:55:17,668 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 11:55:17,668 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 11:55:19,659 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:55:19,660 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 11:55:19,660 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 11:55:19,660 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 11:55:21,534 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:55:21,534 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 11:55:21,534 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 11:55:21,534 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 11:55:23,823 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 11:55:23,823 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 11:55:23,823 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:3b', 'phi3:mini', 'llama3.2:1b', 'gemma2:2b'] - PERSISTENT IN MEMORY
2025-06-30 11:55:23,823 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 11:55:23,823 - system.main - INFO - System initialized successfully
2025-06-30 11:55:23,823 - api.main - INFO - System initialized successfully!
2025-06-30 11:55:24,375 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 11:55:24,389 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://1f7c-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 11:58:42,744 - api.main - INFO - Updated Cloudflare URLs: {'backend': 'https://doom-phd-cause-buf.trycloudflare.com', 'frontend': 'https://knock-titanium-mark-replacing.trycloudflare.com'}
2025-06-30 12:08:25,198 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 12:08:25,202 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 12:08:25,203 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 12:08:25,389 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 12:08:25,691 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 12:08:25,693 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 12:08:25,695 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 12:08:25,696 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 12:08:25,696 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 12:08:26,195 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 12:08:26,197 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 12:08:26,704 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 12:08:26,708 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 12:08:26,708 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 12:08:28,217 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 12:08:28,217 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 12:08:28,217 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 12:08:28,217 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 12:08:30,321 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 12:08:30,321 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 12:08:30,321 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 12:08:30,322 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 12:08:32,276 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 12:08:32,277 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 12:08:32,277 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 12:08:32,277 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 12:08:34,598 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 12:08:34,598 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 12:08:34,599 - backend.ollama_integration - INFO - Currently loaded models: ['llama3.2:1b', 'gemma2:2b', 'phi3:mini', 'llama3.2:3b'] - PERSISTENT IN MEMORY
2025-06-30 12:08:34,599 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 12:08:34,599 - system.main - INFO - System initialized successfully
2025-06-30 12:08:34,599 - api.main - INFO - System initialized successfully!
2025-06-30 12:10:29,876 - api.main - INFO - Shutting down LLM Debate System...
2025-06-30 12:10:29,876 - api.main - INFO - ngrok tunnel stopped
2025-06-30 12:10:42,273 - api.main - INFO - Angular dist folder found at: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 12:10:42,280 - api.main - INFO - Angular static files will be mounted from: C:\Users\wanth\OneDrive\harry\ai\ml\python\llm debate\angular-ui\dist
2025-06-30 12:10:42,280 - api.main - INFO - Initializing LLM Debate System...
2025-06-30 12:10:42,486 - api.main - INFO - Starting ngrok tunnel...
2025-06-30 12:10:42,771 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 12:10:42,771 - system.dynamic_config - INFO - Found 20 available models
2025-06-30 12:10:42,772 - api.main - INFO - Configuration set - Orchestrator: ModelConfig(name='Orchestrator', model='phi3:mini', temperature=0.3, max_tokens=2000, personality='analytical and diplomatic', system_prompt='You are an expert debate orchestrator. Your role is to:\n1. Analyze responses from three debater LLMs\n2. Determine if they have reached consensus\n3. Provide feedback to help them converge\n4. Synthesize final summaries when consensus is reached\nBe objective, thorough, and focus on finding common ground.')
2025-06-30 12:10:42,772 - api.main - INFO - Configuration set - Debaters: [ModelConfig(name='Analytical_Debater', model='llama3.2:1b', temperature=0.6, max_tokens=800, personality='analytical and fact-focused', system_prompt='You are an analytical debater who focuses on facts, data, and logical reasoning.'), ModelConfig(name='Creative_Debater', model='gemma2:2b', temperature=0.7, max_tokens=800, personality='creative and perspective-oriented', system_prompt='You are a creative debater who brings unique perspectives and innovative thinking.'), ModelConfig(name='Pragmatic_Debater', model='llama3.2:3b', temperature=0.8, max_tokens=800, personality='pragmatic and solution-oriented', system_prompt='You are a pragmatic debater focused on practical solutions and real-world applications.')]
2025-06-30 12:10:42,772 - system.main - INFO - Initializing LLM Debate System...
2025-06-30 12:10:43,246 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 12:10:43,246 - backend.ollama_integration - INFO - Ensuring required models are loaded: ['phi3:mini', 'llama3.2:1b', 'gemma2:2b', 'llama3.2:3b']
2025-06-30 12:10:43,734 - httpx - INFO - HTTP Request: GET http://localhost:11434/api/tags "HTTP/1.1 200 OK"
2025-06-30 12:10:43,738 - backend.ollama_integration - INFO - Model phi3:mini needs loading...
2025-06-30 12:10:43,739 - backend.ollama_integration - INFO - Loading model phi3:mini for the FIRST TIME...
2025-06-30 12:10:46,186 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 12:10:46,187 - backend.ollama_integration - INFO - Successfully loaded model phi3:mini - NOW IN MEMORY
2025-06-30 12:10:46,187 - backend.ollama_integration - INFO - Model llama3.2:1b needs loading...
2025-06-30 12:10:46,187 - backend.ollama_integration - INFO - Loading model llama3.2:1b for the FIRST TIME...
2025-06-30 12:10:48,266 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 12:10:48,266 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:1b - NOW IN MEMORY
2025-06-30 12:10:48,267 - backend.ollama_integration - INFO - Model gemma2:2b needs loading...
2025-06-30 12:10:48,267 - backend.ollama_integration - INFO - Loading model gemma2:2b for the FIRST TIME...
2025-06-30 12:10:50,239 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 12:10:50,239 - backend.ollama_integration - INFO - Successfully loaded model gemma2:2b - NOW IN MEMORY
2025-06-30 12:10:50,240 - backend.ollama_integration - INFO - Model llama3.2:3b needs loading...
2025-06-30 12:10:50,240 - backend.ollama_integration - INFO - Loading model llama3.2:3b for the FIRST TIME...
2025-06-30 12:10:52,608 - httpx - INFO - HTTP Request: POST http://localhost:11434/api/generate "HTTP/1.1 200 OK"
2025-06-30 12:10:52,608 - backend.ollama_integration - INFO - Successfully loaded model llama3.2:3b - NOW IN MEMORY
2025-06-30 12:10:52,609 - backend.ollama_integration - INFO - Currently loaded models: ['gemma2:2b', 'llama3.2:3b', 'phi3:mini', 'llama3.2:1b'] - PERSISTENT IN MEMORY
2025-06-30 12:10:52,609 - backend.ollama_integration - INFO - All required models loaded successfully
2025-06-30 12:10:52,609 - system.main - INFO - System initialized successfully
2025-06-30 12:10:52,609 - api.main - INFO - System initialized successfully!
2025-06-30 12:10:56,114 - api.main - INFO - Current Cloudflare URLs: {}
2025-06-30 12:10:56,124 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': 'https://587a-99-58-207-24.ngrok-free.app', 'cloudflare_urls': {}}
2025-06-30 12:11:13,208 - api.main - INFO - Updated Cloudflare URLs: {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}
2025-06-30 12:11:23,213 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}
2025-06-30 12:11:23,227 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}}
2025-06-30 12:11:28,313 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}
2025-06-30 12:11:28,326 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}}
2025-06-30 12:11:38,976 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}
2025-06-30 12:11:38,997 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}}
2025-06-30 12:11:43,611 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}
2025-06-30 12:11:43,625 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}}
2025-06-30 12:11:55,302 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}
2025-06-30 12:11:55,312 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}}
2025-06-30 12:12:47,992 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}
2025-06-30 12:12:48,017 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}}
2025-06-30 12:12:48,746 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}
2025-06-30 12:12:48,757 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}}
2025-06-30 12:12:51,303 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}
2025-06-30 12:12:51,314 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}}
2025-06-30 12:12:52,813 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}
2025-06-30 12:12:52,823 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}}
2025-06-30 12:12:53,807 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}
2025-06-30 12:12:53,817 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}}
2025-06-30 12:12:56,318 - api.main - INFO - Current Cloudflare URLs: {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}
2025-06-30 12:12:56,328 - api.main - INFO - Returning status response: {'initialized': True, 'models_loaded': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b', 'phi3:mini'], 'config': {'max_rounds': 3, 'orchestrator_model': 'phi3:mini', 'debater_models': ['llama3.2:1b', 'gemma2:2b', 'llama3.2:3b'], 'orchestrator_max_tokens': 2000, 'debater_max_tokens': 800}, 'ngrok_url': None, 'cloudflare_urls': {'backend': 'https://manitoba-olive-lights-skins.trycloudflare.com', 'frontend': 'https://memory-herein-grid-cleveland.trycloudflare.com'}}
