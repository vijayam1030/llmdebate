# ğŸ”¥ LLM Debate System - Organized Structure

## ğŸ“ **New Folder Structure**

```
ğŸ“¦ LLM Debate System (Root)
â”œâ”€â”€ ğŸš€ **Main Execution Files** (Keep in Root)
â”‚   â”œâ”€â”€ streamlit_app_session.py      # â­ MAIN UI - True model persistence
â”‚   â”œâ”€â”€ launch_session_ui.py          # Python launcher for session UI
â”‚   â”œâ”€â”€ launch_session_ui.bat         # Windows launcher for session UI
â”‚   â”œâ”€â”€ main.py                       # Core debate system
â”‚   â”œâ”€â”€ config.py                     # Configuration
â”‚   â””â”€â”€ dynamic_config.py             # Dynamic model selection
â”‚
â”œâ”€â”€ ğŸ’» **Backend** (Core Logic)
â”‚   â”œâ”€â”€ agents.py                     # LangChain agents
â”‚   â”œâ”€â”€ models.py                     # Data models
â”‚   â”œâ”€â”€ debate_workflow.py            # LangGraph workflow
â”‚   â”œâ”€â”€ consensus_engine.py           # Consensus analysis
â”‚   â””â”€â”€ ollama_integration.py         # Ollama integration
â”‚
â”œâ”€â”€ ğŸ¨ **UI** (Alternative Interfaces)
â”‚   â”œâ”€â”€ streamlit_app_robust.py       # External process UI (updated)
â”‚   â”œâ”€â”€ streamlit_app_persistent.py   # Thread-based persistence
â”‚   â”œâ”€â”€ streamlit_app_server.py       # Background server UI
â”‚   â”œâ”€â”€ streamlit_app_simple.py       # Simple UI
â”‚   â”œâ”€â”€ streamlit_app_ascii.py        # ASCII-only UI
â”‚   â””â”€â”€ streamlit_launcher.py         # UI launcher
â”‚
â”œâ”€â”€ ğŸ› ï¸ **Scripts** (Utilities & Tests)
â”‚   â”œâ”€â”€ run_small_debate.py           # CLI debate runner
â”‚   â”œâ”€â”€ simple_launcher.py            # Simple CLI launcher
â”‚   â”œâ”€â”€ check_models.py               # Model availability checker
â”‚   â”œâ”€â”€ test_*.py                     # All test files
â”‚   â””â”€â”€ quick_test_3rounds.py         # Quick test script
â”‚
â””â”€â”€ ğŸ“š **Legacy** (Old/Archive Files)
    â”œâ”€â”€ api.py                        # Old API
    â”œâ”€â”€ debate_server.py              # Old server
    â””â”€â”€ run.py                        # Old runner
```

## ğŸš€ **Quick Start**

### **Option 1: Windows Launcher (Easiest)**
```cmd
# Double-click or run:
launch_session_ui.bat
```

### **Option 2: Direct Python**
```cmd
# From the root directory:
streamlit run streamlit_app_session.py
```

### **Option 3: Python Launcher**
```cmd
python launch_session_ui.py
```

## â­ **Recommended UI: `streamlit_app_session.py`**

This is the **BEST** option for model persistence:

âœ… **True Model Persistence** - Models stay loaded between debates  
âœ… **Session State** - No external processes or servers  
âœ… **Simple & Reliable** - No complex dependencies  
âœ… **Fast Subsequent Debates** - Models already loaded  
âœ… **Memory Efficient** - Optimal resource usage  

## ğŸ”§ **Alternative UIs (in `/ui/` folder)**

- **`streamlit_app_robust.py`** - External process (conflict-free but reloads models)
- **`streamlit_app_persistent.py`** - Thread-based persistence
- **`streamlit_app_server.py`** - Background server approach
- **`streamlit_app_simple.py`** - Basic UI without persistence

## ğŸ“‹ **System Requirements**

- **Python 3.8+**
- **Ollama** running locally (`ollama serve`)
- **Small models installed** (under 4GB each):
  ```bash
  ollama pull llama3.2:3b
  ollama pull gemma2:2b
  ollama pull phi3:mini
  ollama pull tinyllama:1.1b
  ```

## ğŸ—ï¸ **Benefits of New Structure**

1. **ğŸ¯ Clean Root** - Only essential files in main directory
2. **ğŸ“ Organized** - Logical grouping by function
3. **â­ Best UI First** - Session UI prominently featured
4. **ğŸ”§ Easy Maintenance** - Clear separation of concerns
5. **ğŸš€ Simple Launch** - One-click Windows launcher

## ğŸ’¡ **Usage Tips**

- **Start with**: `launch_session_ui.bat` (Windows) or `streamlit run streamlit_app_session.py`
- **For CLI**: Use `python scripts/run_small_debate.py "your question"`
- **For testing**: Check files in `/scripts/` folder
- **For development**: Backend logic in `/backend/` folder

---

**ğŸ¯ Focus: Session-based UI provides the best experience with true model persistence!**
