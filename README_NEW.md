# ğŸ¯ LLM Debate System (Small Models Only)

**A clean, organized AI debate system using only small local Ollama models (<4GB each)**

## ğŸš€ Quick Start

### **Windows (Easiest)**
```bash
# Double-click:
bin\launch.bat

# Or from command line:
streamlit run streamlit_app_session.py
```

### **Any Platform**
```bash
# Install dependencies:
pip install -r config/requirements.txt

# Run the web interface:
streamlit run streamlit_app_session.py
```

## ğŸ“ Project Structure

```
ğŸ“ Root (Clean!)
â”œâ”€â”€ streamlit_app_session.py    â­ Main web interface
â”œâ”€â”€ README.md                   ğŸ“– This file
â”œâ”€â”€ .env                        ğŸ”§ Environment config
â””â”€â”€ .gitignore

ğŸ“ /backend/                    ğŸ”§ Core system logic
â”œâ”€â”€ agents.py                   ğŸ¤– AI agents
â”œâ”€â”€ models.py                   ğŸ“Š Data models  
â”œâ”€â”€ debate_workflow.py          ğŸ”„ Debate flow
â”œâ”€â”€ consensus_engine.py         ğŸ¯ Agreement analysis
â””â”€â”€ ollama_integration.py       ğŸ”— Model interface

ğŸ“ /config/                     âš™ï¸ Configuration
â”œâ”€â”€ config.py                   ğŸ“‹ Settings
â”œâ”€â”€ dynamic_config.py           ğŸ”„ Auto-config
â”œâ”€â”€ main.py                     ğŸ¯ Core system
â”œâ”€â”€ requirements.txt            ğŸ“¦ Dependencies
â””â”€â”€ launch_session_ui.py        ğŸš€ Python launcher

ğŸ“ /bin/                        ğŸ’» Executables  
â”œâ”€â”€ launch.bat                  ğŸ–±ï¸ Easy Windows launcher
â”œâ”€â”€ launch_session_ui.bat       ğŸ¯ Session UI launcher
â””â”€â”€ *.bat, *.sh                 ğŸ”§ Other scripts

ğŸ“ /docs/                       ğŸ“š Documentation
â”œâ”€â”€ ORGANIZATION_COMPLETE.md    âœ… Structure guide
â”œâ”€â”€ WORKING_SETUP.md           ğŸ”§ Setup guide
â””â”€â”€ *.md                       ğŸ“– Other docs

ğŸ“ /logs/                       ğŸ“œ Log files
ğŸ“ /scripts/                    ğŸ”§ Utilities & tests
ğŸ“ /ui/                         ğŸ¨ Alternative UIs
ğŸ“ /legacy/                     ğŸ“¦ Archived code
```

## âœ¨ Key Features

- âœ… **Only Small Models** - Under 4GB each (tinyllama, phi3:mini, gemma2:2b, llama3.2:3b)
- âœ… **True Model Persistence** - Models stay loaded between debates  
- âœ… **Max 3 Rounds** - Efficient, focused debates
- âœ… **Clean Interface** - Streamlit web UI
- âœ… **Windows Compatible** - No Unicode/encoding issues
- âœ… **Organized Structure** - Clear separation of concerns

## ğŸ”§ Configuration

The system auto-configures using small models. Manual config in `/config/config.py` if needed.

**Recommended Models** (install with `ollama pull <model>`):
- `tinyllama:1.1b` - Ultra-fast, 1.1GB
- `phi3:mini` - Compact, 2.3GB  
- `gemma2:2b` - Efficient, 1.6GB
- `llama3.2:3b` - Balanced, 2GB

## ğŸ­ Usage

1. **Launch**: Double-click `bin\launch.bat` or run `streamlit run streamlit_app_session.py`
2. **Ask**: Enter your debate question
3. **Watch**: 3 AI agents debate the topic
4. **Results**: Get consensus analysis and summary

## ğŸ—ï¸ Development

- **Backend**: Core logic in `/backend/`
- **Config**: Settings in `/config/`  
- **Tests**: Utilities in `/scripts/`
- **Docs**: Guides in `/docs/`

---

**Clean, fast, and efficient AI debates with local models! ğŸ‰**
